\cutname{jingle.html}

The tool \jingle{} is a litmus test convertor.
Users may define convertions rules of instructions and have the tool convert
tests meant for some architecture to ones for another.

\section{Writing convertions rules}
For the tool to know how to convert each instruction as for allowing users to
freely define optimisations and workarounds, a file gathering all the allowed
convertions is necessary.

\subsection{Minimal conversions}
Such files are writen in a simple manner, first we tell from which to which
architectures the file stands for, then associate patterns of instructions
for the source architecture to assumed equivalent patterns
for the target one.\newline
Since the main purpose of the tool is the use of our abstract language LISA,
the examples in this tutorial will be conversions from LISA to a concrete
assembly language (namely AArch64).

\begin{verbatim}
LISA to AArch64

"r[] %x %y" -> "LDR %x,[%y]"

"w[] %x %y" -> "STR %y,[%x]"
\end{verbatim}

The first line \textbf{must} be of the form
\verb+[source_arch] to [target_arch]+.\newline
Each rule is code from the given architectures in double-quotes,
separated by an arrow (or the key word \verb+maps to+).
Such code is the same as seen in any well-formed tests with some
abstract expressions.

There would be no point to have a rule for each different usage of a same
usage of instruction therefore we can express various elements in the code
by identification.
Representing registers with symbolics is the most common use of identification:
in the rules above, all that matters is to have the registers \verb+x+
and \verb+y+ of the source program be at the right place in the converted test.

\subsection{Multiple instructions pattern and other identifications}
Sometimes, a single instruction with a couple of symbolic register
is not nearly enough to express every conversion in a good fashion.

A single rule ought to be enough for anybody to understand:
\begin{verbatim}
"w[] x &c" -> "MOV %tmp,&c;
               STR %tmp,[%x]"
\end{verbatim}

In AArch64, it is not possible to directly store a constant value in memory,
thus we have to express the LISA instruction in two AArch64 instructions
with a register picked on the fly.

This example illustrates at the same time:
\begin{itemize}
\item how to identify addresses for languages which can directly mention it,
such as LISA, where they are simply represented by a name. A address can
be converted to a register which would contain the said address value.
\item how to identify constants. For every assembly languages, a constant can be
represented by a name preceded by the character \verb+&+.
\item how to define patterns with multiple instructions by separating them
with a semicolon.
\item that identifiers are not necessarily bound to appear on both side of
a rule if they are only needed in one.
\end{itemize}

In case of ambiguity, \jingle{} choose the rule to apply according to their order
in the file: the higher the rule, the higher its priority.

\subsection{Multiple level pattern and structured languages}
The last section covers what is necessary to convert most tests from any assembly language
to another. However, we might want to allow our tool to work on higher level languages.

The suite currently support a relevant subset of the C language.
That means our tool must not only convert sequences of instructions but also
potential control structures.

For this purpose, we must allow the expression of chunks of code:

\begin{verbatim}
C to LISA

"if(x==constvar:c)
   codevar:t
 else
   codevar:e"     -> "mov %test (eq %x &c);
                      b[] %test then;
                      codevar:e
                      b[] 1 end;
                      then : codevar:t
                      end :"
\end{verbatim}

This awfully looks like a compilation process (and it is!), but in practice,
conditionals are used to express control dependencies which can have a much
simpler form in assembly code.

Now, the important point here is the use of \verb+codevar:+ to state that
arbitrary code is expected. Such code will also be converted by the same
given set of rules, thus allowing us to convert arbitrarily deep code.

Notice that labels too are subject to identification and that is
perfectly fine to end a pattern with it since the tool will see
a nop-like instruction.

The special keyword \verb+constvar:+ is used in C for the obvious reason
that \verb+&+ has an entirely different meaning.

\section{Rewriting algorithm}
Now with a well defined file, we can let the burden of converting our
thousands of tests to \jingle{}.

In order to fully understand its behavior, we shall explore
more in detail its mechanisms.

\subsection{Rule application and substitution mechanism}
The rules we define are nothing but generic patterns, for them
to hold any meaning we have to find an application in the source program.
Such application is simply an instance of the conversion of a part
of the source program. The source part must match the pattern in the
left side of the rule, the instance of the conversion is the code
defined in the right side plus a set of what we call substitutions.

The substitutions are the link between the identifiers in the rule
and their actual representations in both the source and target programs.

To roughly formalize,
$App(R, P) = \left(R_{right}, \left\{\left(id,Src_{id},Tgt_{id}\right) \mid id \text{is an identifier of} R\right\}\right)$
would be the application of the rule $R$ on a part of the source program $P$,
where $P$ is a possible instance of $R_{left}$ and $Src_{id}$ ($Tgt_{id}$) is the source
(respectively the target) representation.

\subsection{Linear processing}
Now that we have a first step of local rewriting, we want to convert an entire
program. Thanks to relative simplicity of the supported languages, decomposing
a source program linearly is a good enough approach for our needs.

The process can be divided in two steps:
\begin{itemize}
\item The actual decomposition of the source program into a sequence of rule applications.
\item The recomposition into a target program by instancing the previous sequence
according to its substitutions.
\end{itemize}

\paragraph{Decomposing}
Starting at the beginning, the first rule (in the priority order) to match
a part of the program, in other words, the first possible application od the rule,
is considered and the process repeat itself on the rest of the program until
eventually it has been completely consumed. We end up with a sequence of
patterns and associed substitutions.

A recursive definition would be:
\begin{align*}
& Decomp(\bullet,Rs) = \bullet && where R is the highest possible element of the rule set Rs\\
& Decomp(P | Source_{rest},Rs) = App(R,P) | Decomp(Source_{rest},Rs) && and P \in Instances(R_{left})
\end{align*}

Of course, we assume the given rule set is sufficient to assure that all part of the
program will be matched. If not, users have to refine it.

\paragraph{Recomposing}
With the result of the previous step, which already looks like a converted program,
we have to actually substitute the abstract identifiers for their representation
in the target language given by the associed substitutions, for each part.
Then simply append the results to one another in order.

\begin{align*}
& Recomp(\bullet) = \bullet\\
& Recomp((R_{right}, Subs) | Parts_{rest}) = R_{right}[\{id \mapsto Tgt_{id} \mid (id,Src_{id},Tgt_{id})\in Subs\}] | Recomp(Parts_{rest})
\end{align*}

\begin{remark}
The substitutions can also include code since patterns allow it. This code is converted following the same
procedure thus any code substitution have the form $(id,Src_{id},Recomp(Decomp(Src_{id},Rs)))$.
\end{remark}

\subsection{Representation coherence and environment}
There is one key aspect that we have yet to cover. Until now, the coherence between the source et target representation of
a substitution in regards to the others was assumed, i.e.:
\begin{equation*}
\forall id_1,id_2, \; Src_{id_1} = Src_{id_2} \Rightarrow Tgt_{id_1} = Tgt_{id_2}
\end{equation*}
However, if this property in the substitutions of a single application is given by the rule itself,
ensuring it between different applications is not as obvious because it would not make sense to
compare pattern identifiers from different rules. Thus, we need to keep track of any $Src,Tgt$
association made by applications through the whole program.

To do so, we define a global environment that preserves the property by delivering a target
language representation for each source value:
\begin{align*}
& Get\_repr(\Gamma,Src_{id}) = fresh\_repr(Src_{id}) & if Src_{id} \notin \Gamma\\
& Get\_repr(Src_{id}\mapsto Tgt_{id}\mid\Gamma,Src_{id}) = Tgt_{id} &
\end{align*}

Nothing exotic here, this is a part of the application of a rule as it is supposed
to be the only safe way to obtain a target representation.

\section{Usage of \jingle{}}

\subsection{Arguments}
The command \jingle{} handles its arguments as file names, just as \herd{}.
Those files are either a single litmus test when having extension .litmus,
or a list of file names when prefixed by @.

\subsection{Options}
There is one option that must always be used:

\begin{description}
\item[{\tt -theme <name>}] Read the conversion rules file <name>.
By convention, such files have the extention \verb+.theme+.
\end{description}

\paragraph{General behavior}
\begin{description}
\item[{\tt -v}] Be verbose.
\item[{\tt -o <dest>}] Instead of printing the result on the standard output,
output test files in the existing <dest> directory. Those files have the same
name as the input tests.
\end{description}

\subsection{Regarding conversion errors}
When the tool fails to find a conversion in a program, it will print the remaining instructions.
It makes easy for the user to pin down missing rules in his \verb+.theme+ file
as the first instruction printed is likely the one that cannot be matched.

Using those error might be helpful to build such file instead of trying to figure it out
as a whole beforehand.
