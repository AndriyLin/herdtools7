<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="generator" content="hevea 2.27">
<link rel="stylesheet" type="text/css" href="diy.css">
<title>Running tests with litmus7</title>
</head>
<body>
<a href="diy001.html"><img src="previous_motif.gif" alt="Previous"></a>
<a href="index.html"><img src="contents_motif.gif" alt="Up"></a>
<a href="gen.html"><img src="next_motif.gif" alt="Next"></a>
<hr>
<table class="center"><tr><td><h1 class="part" id="part:litmus">Part&#XA0;I<br>
Running tests with <span class="c007">litmus7</span></h1></td></tr>
</table>
<ul>
<li><a href="litmus.html#sec3">A tour of&#XA0;<span class="c007">litmus7</span></a>
<ul>
<li><a href="litmus.html#litmus%3Asimple">A  simple run</a>
</li><li><a href="litmus.html#cross">Cross compilation</a>
</li><li><a href="litmus.html#sec6">Running several tests at once</a>
</li></ul>
</li><li><a href="litmus.html#litmus%3Acontrol">Controlling test parameters</a>
<ul>
<li><a href="litmus.html#sec%3Aarch">Architecture of tests</a>
</li><li><a href="litmus.html#sec%3Aaffinity">Affinity</a>
</li><li><a href="litmus.html#exec%3Acontrol">Controlling executable files</a>
</li></ul>
</li><li><a href="litmus.html#advanced%3Acontrol">Advanced control of test parameters</a>
<ul>
<li><a href="litmus.html#timebase">Timebase synchronisation mode</a>
</li><li><a href="litmus.html#preload%3Acustom">Advanced prefetch control</a>
</li></ul>
</li><li><a href="litmus.html#sec23">Usage of <span class="c007">litmus7</span></a>
<ul>
<li><a href="litmus.html#sec24">Arguments</a>
</li><li><a href="litmus.html#sec25">Options</a>
</li><li><a href="litmus.html#sec31">Configuration files</a>
</li></ul>
</li></ul>
<p>
Traditionally, a <em>litmus test</em> is a small parallel program designed
to exercise the memory model of a parallel, shared-memory, computer.
Given a litmus test in assembler (X86, Power or ARM) <span class="c007">litmus7</span>
runs the test.</p><p>Using <span class="c007">litmus7</span> thus requires a parallel machine,
which must additionally feature <span class="c007">gcc</span> and the <span class="c007">pthreads</span> library.
Our tool <span class="c007">litmus7</span> has some limitations especially
as regards recognised instructions.
Nevertheless, <span class="c007">litmus7</span> should accept all tests
produced by the companion test generators (see Part&#XA0;<a href="gen.html#part%3Adiy">II</a>)
and has been successfully used on Linux, MacOS, AIX and&#XA0;Android.</p>
<h2 class="section" id="sec3">1&#XA0;&#XA0;A tour of&#XA0;<span class="c007">litmus7</span></h2>
<h3 class="subsection" id="litmus:simple">1.1&#XA0;&#XA0;A  simple run</h3>
<p>
<a id="SB">Consider</a> the following (rather classical, store buffering)
<a href="SB.litmus"><span class="c004">SB.litmus</span></a> litmus test for X86:
</p><pre class="verbatim">X86 SB
"Fre PodWR Fre PodWR"
{ x=0; y=0; }
 P0          | P1          ;
 MOV [x],$1  | MOV [y],$1  ;
 MOV EAX,[y] | MOV EAX,[x] ;
locations [x;y;]
exists (0:EAX=0 /\ 1:EAX=0)
</pre><p>
A litmus test source has three main sections:
</p><ol class="enumerate" type=1><li class="li-enumerate">
The initial state defines the initial values of registers
and memory locations. Initialisation to zero may be omitted.
</li><li class="li-enumerate">The code section defines the code to be run concurrently
&#X2014; above there are two threads.
Yes we know, our X86 assembler syntax is a mistake.
</li><li class="li-enumerate">The final condition applies to the final values
of registers and memory locations.
</li></ol><p><a id="x86:classic"></a>Run the test by
(complete <a href="SB.log">run log</a>):
</p><pre class="verbatim">% litmus7 SB.litmus
%%%%%%%%%%%%%%%%%%%%%%%%%
% Results for SB.litmus %
%%%%%%%%%%%%%%%%%%%%%%%%%
X86 SB
"Fre PodWR Fre PodWR"

{x=0; y=0;}

 P0          | P1          ;
 MOV [x],$1  | MOV [y],$1  ;
 MOV EAX,[y] | MOV EAX,[x] ;

exists (0:EAX=0 /\ 1:EAX=0)
Generated assembler
#START _litmus_P1
 movl $1,(%r10)
 movl (%r9),%eax
#START _litmus_P0
 movl $1,(%r9)
 movl (%r10),%eax

Test SB Allowed
Histogram (4 states)
40    *&gt;0:EAX=0; 1:EAX=0;
499923:&gt;0:EAX=1; 1:EAX=0;
500009:&gt;0:EAX=0; 1:EAX=1;
28    :&gt;0:EAX=1; 1:EAX=1;
Ok

Witnesses
Positive: 40, Negative: 999960
Condition exists (0:EAX=0 /\ 1:EAX=0) is validated
Hash=7dbd6b8e6dd4abc2ef3d48b0376fb2e3
Observation SB Sometimes 40 999960
Time SB 0.44
...
</pre><p>The litmus test is first reminded, followed by actual assembler
&#X2014; the machine is a 64&#XA0;bits one, in-line address references disappeared,
registers may change, and assembler syntax is now more familiar.
The test has run one million times, producing one million final states,
or <em>outcomes</em> for the registers <code>EAX</code> of threads <span class="c010">P</span><sub>0</sub> and&#XA0;<span class="c010">P</span><sub>1</sub>.
The test run validates the condition, with 40&#XA0;positive witnesses.</p>
<h3 class="subsection" id="cross">1.2&#XA0;&#XA0;Cross compilation</h3>
<p>With option <span class="c004">-o &lt;name.tar&gt;</span>, <span class="c007">litmus7</span> does not run the test.
Instead, it produces a tar archive that contains the C&#XA0;sources for
the test.</p><p>Consider <a href="SB-PPC.litmus"><span class="c004">SB-PPC.litmus</span></a>, a Power version
of the previous test:
</p><pre class="verbatim">PPC SB-PPC
"Fre PodWR Fre PodWR"
{
0:r2=x; 0:r4=y;
1:r2=y; 1:r4=x;
}
 P0           | P1           ;
 li r1,1      | li r1,1      ;
 stw r1,0(r2) | stw r1,0(r2) ;
 lwz r3,0(r4) | lwz r3,0(r4) ;
exists (0:r3=0 /\ 1:r3=0)
</pre><p>Our target machine (ppc) runs MacOS, which we specify with the <span class="c004">-os</span>&#XA0;option:
</p><pre class="verbatim">% litmus7 -o /tmp/a.tar -os mac SB-PPC.litmus
% scp /tmp/a.tar ppc:/tmp
</pre><p>Then, on the remote machine ppc:
</p><pre class="verbatim">ppc% mkdir SB &amp;&amp; cd SB
ppc% tar xf /tmp/a.tar
ppc% ls
comp.sh  Makefile  outs.c  outs.h  README.txt  run.sh  SB-PPC.c  show.awk  utils.c  utils.h
</pre><p>Test is compiled by the shell script <span class="c004">comp.sh</span> (or
by (Gnu) <span class="c004">make</span>, at user&#X2019;s choice) and run by the shell script
<span class="c004">run.sh</span>:
</p><pre class="verbatim">ppc% sh comp.sh
ppc% sh run.sh
...
Test SB-PPC Allowed
Histogram (3 states)
1784  *&gt;0:r3=0; 1:r3=0;
498564:&gt;0:r3=1; 1:r3=0;
499652:&gt;0:r3=0; 1:r3=1;
Ok

Witnesses
Positive: 1784, Negative: 998216
Condition exists (0:r3=0 /\ 1:r3=0) is validated
Hash=4edecf6abc507611612efaecc1c4a9bc
Observation SB-PPC Sometimes 1784 998216
Time SB-PPC 0.55
...
</pre><p>(Complete <a href="SB-PPC.log">run log</a>.) As we see, the condition validates also on Power.
Notice that compilation produces an executable file, <span class="c004">SB-PPC.exe</span>,
which can be run directly, for a less verbose output.</p>
<h3 class="subsection" id="sec6">1.3&#XA0;&#XA0;Running several tests at once</h3>
<p><a id="stfw"></a>Consider the additional test&#XA0;<a href="STFW-PPC.litmus"><span class="c004">STFW-PPC.litmus</span></a>:
</p><pre class="verbatim">PPC STFW-PPC
"Rfi PodRR Fre Rfi PodRR Fre"
{
0:r2=x; 0:r5=y;
1:r2=y; 1:r5=x;
}
 P0           | P1           ;
 li r1,1      | li r1,1      ;
 stw r1,0(r2) | stw r1,0(r2) ;
 lwz r3,0(r2) | lwz r3,0(r2) ;
 lwz r4,0(r5) | lwz r4,0(r5) ;
exists
(0:r3=1 /\ 0:r4=0 /\ 1:r3=1 /\ 1:r4=0)
</pre><p>To compile the two tests together,
we can give two file names as arguments to litmus:
</p><pre class="verbatim">$ litmus7 -o /tmp/a.tar -os mac SB-PPC.litmus STFW-PPC.litmus 
</pre><p>Or, more conveniently, list the litmus sources in a file whose
name starts with <span class="c004">@</span>:
</p><pre class="verbatim">$ cat @ppc
SB-PPC.litmus
STFW-PPC.litmus
$ litmus7 -o /tmp/a.tar -os mac @ppc
</pre><p>To run the test on the remote ppc machine, the same sequence
of commands as in the one test case applies:
</p><pre class="verbatim">ppc% tar xf /tmp/a.tar &amp;&amp; make &amp;&amp; sh run.sh
...
Test SB-PPC Allowed
Histogram (3 states)
1765  *&gt;0:r3=0; 1:r3=0;
498741:&gt;0:r3=1; 1:r3=0;
499494:&gt;0:r3=0; 1:r3=1;
Ok

Witnesses
Positive: 1765, Negative: 998235
Condition exists (0:r3=0 /\ 1:r3=0) is validated
Hash=4edecf6abc507611612efaecc1c4a9bc
Observation SB-PPC Sometimes 1765 998235
Time SB-PPC 0.57
...
Test STFW-PPC Allowed
Histogram (4 states)
480   *&gt;0:r3=1; 0:r4=0; 1:r3=1; 1:r4=0;
499560:&gt;0:r3=1; 0:r4=1; 1:r3=1; 1:r4=0;
499827:&gt;0:r3=1; 0:r4=0; 1:r3=1; 1:r4=1;
133   :&gt;0:r3=1; 0:r4=1; 1:r3=1; 1:r4=1;
Ok

Witnesses
Positive: 480, Negative: 999520
Condition exists (0:r3=1 /\ 0:r4=0 /\ 1:r3=1 /\ 1:r4=0) is validated
Hash=92b2c3f6332309325000656d0632131e
Observation STFW-PPC Sometimes 480 999520
Time STFW-PPC 0.56
...
</pre><p>(Complete <a href="SB-PPC2.log">run log</a>.) Now, the output of <span class="c004">run.sh</span> shows the result of two tests.</p>
<h2 class="section" id="litmus:control">2&#XA0;&#XA0;Controlling test parameters</h2>
<p>
Users can control some of testing conditions.
Those impact efficiency and outcome variability.</p><p>Sometimes one looks for a particular outcome
&#X2014; for instance, one may seek to get the
outcome <code>0:r3=1; 1:r3=1;</code> that is missing
in the previous experiment for test&#XA0;<span class="c008">SB-PPC</span>.
To that aim, varying test conditions may help.</p>
<h3 class="subsection" id="sec:arch">2.1&#XA0;&#XA0;Architecture of tests</h3>
<p>Consider a test <span class="c004">a.litmus</span>
designed to run on <span class="c010">t</span> threads <span class="c010">P</span><sub>0</sub>,&#X2026;, <span class="c010">P</span><sub><span class="c010">t</span>&#X2212;1</sub>.
The structure of the executable <span class="c004">a.exe</span> that performs
the experiment is as follows:
</p><ul class="itemize"><li class="li-itemize">
<a id="defn"></a><a id="defa"></a>So as to benefit from parallelism, 
we run <span class="c010">n</span> = max(1,<span class="c010">a</span>/<span class="c010">t</span>) (integer division)
tests concurrently on a machine where <span class="c010">a</span>&#XA0;logical processors are available.
</li><li class="li-itemize"><a id="defr"></a>Each of these (identical)
tests consists in repeating <span class="c010">r</span> times
the following sequence:
<ul class="itemize"><li class="li-itemize">
Fork &#XA0;<span class="c010">t</span> (POSIX) threads <span class="c010">T</span><sub>0</sub>,&#X2026; <span class="c010">T</span><sub><span class="c010">t</span>&#X2212;1</sub>
for executing <span class="c010">P</span><sub>0</sub>,&#X2026;, <span class="c010">P</span><sub><span class="c010">t</span>&#X2212;1</sub>.
Which thread executes which code is either fixed, or changing,
controlled by the <em>launch mode</em>.
In our experience, the launch mode has marginal impact.<p>In <em>cache mode</em> the <span class="c010">T</span><sub><span class="c010">k</span></sub> threads are re-used.
As a consequence, <span class="c010">t</span>&#XA0;threads only are forked.</p></li><li class="li-itemize"><a id="defs"></a>Each thread <span class="c010">T</span><sub><span class="c010">k</span></sub> executes a loop of size&#XA0;<span class="c010">s</span>.
Loop iteration number&#XA0;<span class="c010">i</span> executes the code of <span class="c010">P</span><sub><span class="c010">k</span></sub> (in fixed mode)
and saves
the final contents of its observed registers in some arrays indexed by&#XA0;<span class="c010">i</span>.
Furthermore, still for iteration&#XA0;<span class="c010">i</span>, memory location&#XA0;<span class="c010">x</span> is in fact
an array cell.<p><a id="defmemorymode"></a><a id="defstride"></a>How this array cell is accessed depends
upon the <em>memory mode</em>.
In <em>direct mode</em> the array cell is accessed directly as&#XA0;<span class="c010">x</span>[<span class="c010">i</span>];
as a result, cells are accessed sequentially and false sharing effects
are likely.
In <em>indirect mode</em> the array cell is accessed by the means of a
shuffled array of pointers;
as a result we observed a much greater variability of outcomes.
Additionally, the increment of the main loop (of size&#XA0;<span class="c010">s</span>)
can be set to a value or <em>stride</em> different from the default of&#XA0;one.
Running a test several times with changing the stride value also
proved quite effective in favouring outcome variability.</p><p><a id="defpreload"></a>If the <em>random preload mode</em> is enabled,
a preliminary loop of size&#XA0;<span class="c010">s</span> reads
a random subset of the memory locations accessed by&#XA0;<span class="c010">P</span><sub><span class="c010">k</span></sub>.
Preload have a noticeable effect and teh random preload mode is
enabled by default.
Starting from version&#XA0;5.0, we provide a more precise control
over preloading memory locations &#X2014; See Sec.&#XA0;<a href="#preload%3Acustom">3.2</a>.</p><p><a id="defsynchronisation"></a>The iterations performed
by the different threads&#XA0;<span class="c010">T</span><sub><span class="c010">k</span></sub> may be unsynchronised,
exactly synchronised by a pthread based barrier, or approximately synchronised
by specific code.
Absence of synchronisation may be interesting when <span class="c010">t</span> exceeds&#XA0;<span class="c010">a</span>.
As a matter of fact, in this situation,
any kind of synchronisation leads to prohibitive running times.
However, for a large value of parameter&#XA0;<span class="c010">s</span> and small <span class="c010">t</span> we have observed
spontaneous concurrent execution of some iterations amongst many.
Pthread based barriers are exact but they are slow
and in fact offers poor synchronisation for short code sequences.
The approximate synchronisation is thus the preferred technique.</p><p>Starting from version 5.0, we provide a slightly altered
user synchronisation mode: <em>userfence</em>, which alters
user mode by executing memory fences to speedup write propagation.
The new mode features overall better synchronisation, yielding dramatic
improvements on some examples. However,
outcome variability may suffer from this more accurate synchronisation,
hence user mode remains the default.</p><p><a id="timebase:intro"></a>More importantly,
we provide an additional exact, <em>timebase</em>
synchronisation
technique: test threads will first synchronise using polling synchronisation
barrier code,
agree on a target timebase<sup><a id="text1" href="#note1">1</a></sup> value and then loop
reading the timebase until it exceeds the target value.
This technique yields very good synchronisation and allows
fine synchronisation tuning by assigning different starting delays to
different threads &#X2014; see Sec.&#XA0;<a href="#timebase">3.1</a>.
As ARM does not provide timebase counters,
notice that &#X201C;timebase&#X201D; synchronisation for ARM silently degrades
to synchronisation by the means of the polling synchronisation barrier.</p></li><li class="li-itemize">Wait for the <span class="c010">t</span> threads to terminate and collect outcomes
in some histogram like structure.
</li></ul>
</li><li class="li-itemize">Wait for the <span class="c010">n</span>&#XA0;tests to terminate and sum their histograms.
</li></ul><p>Hence, running <span class="c004">a.exe</span> produces <span class="c010">n</span> &#XD7; <span class="c010">r</span> &#XD7; <span class="c010">s</span> outcomes.
Parameters <span class="c010">n</span>, <span class="c010">a</span>, <span class="c010">r</span> and&#XA0;<span class="c010">s</span> can first be set directly while
invoking <span class="c004">a.exe</span>, using the appropriate command line options.
For instance, assuming <span class="c010">t</span>=2,
<code>./a.exe -a 201 -r 10k -s 1</code> and <code>./a.exe -n 1 -r 1 -s 1M</code>
will both produce one million outcomes, but the latter is probably
more efficient.
If our machine has 8&#XA0;cores,
<code>./a.exe -a 8 -r 1 -s 1M</code> will yield 4&#XA0;millions outcomes,
in a time that we hope not to exceed too much the one experienced
with&#XA0;<code>./a.exe -n 1</code>.
Also observe that the memory allocated is roughly proportional
to <span class="c010">n</span> &#XD7; <span class="c010">s</span>, while the number of <span class="c010">T</span><sub><span class="c010">k</span></sub>&#XA0;threads created will be
<span class="c010">t</span> &#XD7; <span class="c010">n</span> &#XD7; <span class="c010">r</span> (<span class="c010">t</span> &#XD7; <span class="c010">n</span> in cache mode).
The <span class="c004">run.sh</span> shell script transmits its command line to all
the executable (<span class="c004">.exe</span>) files
it invokes, thereby providing a convenient means
to control testing condition for several tests.
Satisfactory test parameters are found by experimenting and
the control of executable files by command line options is designed for
that purpose.</p><p>Once satisfactory parameters are found, it is a nuisance to repeat them
for every experiment.
Thus, parameters <span class="c010">a</span>, <span class="c010">r</span> and&#XA0;<span class="c010">s</span> can also be set while invoking litmus,
with the same command line options. In fact those settings command
he default values of <span class="c004">.exe</span>&#XA0;files controls.
Additionally, the synchronisation technique for iterations,
the memory mode, and several others compile time parameters
can be selected by appropriate <span class="c007">litmus7</span> command line options.
Finally, users can record frequently used parameters in configuration files.</p>
<h3 class="subsection" id="sec:affinity">2.2&#XA0;&#XA0;Affinity</h3>
<p>We view affinity as
a scheduler property that binds a (software, POSIX) thread to
a given (hardware) <em>logical processor</em>.
In the most simple situation a logical processor is a core.
However in the presence of hyper-threading (x86) or simultaneous multi threading
(SMT, Power) a given core can host several logical processors.</p>
<h4 class="subsubsection" id="sec10">2.2.1&#XA0;&#XA0;Introduction to affinity</h4>
<p>
In our experience,
binding the threads of test programs to selected logical processors
yields significant speedups and, more importantly, greater outcome variety.
We illustrate the issue by the means of an example.</p><p>We consider the test&#XA0;<a href="ppc-iriw-lwsync.litmus"><span class="c004">ppc-iriw-lwsync.litmus</span></a>:
</p><pre class="verbatim">PPC ppc-iriw-lwsync
{
0:r2=x; 1:r2=x; 1:r4=y;
2:r4=y; 3:r2=x; 3:r4=y;
}
 P0           | P1           | P2           | P3           ;
 li r1,1      | lwz r1,0(r2) | li r1,1      | lwz r1,0(r4) ;
 stw r1,0(r2) | lwsync       | stw r1,0(r4) | lwsync       ;
              | lwz r3,0(r4) |              | lwz r3,0(r2) ;
exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0)
</pre><p>
The test consists of four threads.
There are two writers (P0 and P2) that write the value
one into two different locations (<span class="c004">x</span> and <span class="c004">y</span>),
and two readers that read the contents of <span class="c004">x</span> and <span class="c004">y</span>
in different orders &#X2014; P1 reads <span class="c004">x</span> first, while P3 reads
<span class="c004">y</span> first.
The load instructions <span class="c004">lwz</span> in reader threads are separated
by a lightweight barrier instruction&#XA0;<span class="c004">lwsync</span>.
The final condition <code>exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0)</code>
characterises the situation where the reader threads see the writes
by P0 and P2 in opposite order.
The corresponding outcome <code>1:r1=1; 1:r3=0; 3:r1=1; 3:r3=0;</code>
is the only non-sequential consistent (non-SC, see Part&#XA0;<a href="gen.html#part%3Adiy">II</a>) possible outcome.
By any reasonable memory model for Power,
one expects the condition to validate,
<em>i.e.</em> the non-SC outcome to show up.</p><p>The tested machine
<a href="http://www.idris.fr/su/Scalaire/vargas/hw-vargas.html"><span class="c004">vargas</span></a>
is a Power&#XA0;6 featuring 32&#XA0;cores (<em>i.e.</em>
64 logical processors, since SMT is enabled) and running AIX in 64 bits mode.
So as not to disturb other users, we run only one instance of the test,
thus specifying four available processors.
The <span class="c007">litmus7</span> tool is absent on <span class="c004">vargas</span>.
All these conditions command the following invocation of <span class="c007">litmus7</span>,
performed on our local machine:
</p><pre class="verbatim">$ litmus7 -r 1000 -s 1000 -a 4 -os aix -ws w64 ppc-iriw-lwsync.litmus -o ppc.tar
$ scp ppc.tar vargas:/var/tmp
</pre><p>On <span class="c004">vargas</span> we unpack the archive and compile the test:
</p><pre class="verbatim">vargas% tar xf /var/tmp/ppc.tar &amp;&amp; sh comp.sh
</pre><p>Then we run the test:
</p><pre class="verbatim">vargas% ./ppc-iriw-lwsync.exe
Test ppc-iriw-lwsync Allowed
Histogram (15 states)
163674:&gt;1:r1=0; 1:r3=0; 3:r1=0; 3:r3=0;
34045 :&gt;1:r1=1; 1:r3=0; 3:r1=0; 3:r3=0;
40283 :&gt;1:r1=0; 1:r3=1; 3:r1=0; 3:r3=0;
95079 :&gt;1:r1=1; 1:r3=1; 3:r1=0; 3:r3=0;
33848 :&gt;1:r1=0; 1:r3=0; 3:r1=1; 3:r3=0;
72201 :&gt;1:r1=0; 1:r3=1; 3:r1=1; 3:r3=0;
32452 :&gt;1:r1=1; 1:r3=1; 3:r1=1; 3:r3=0;
43031 :&gt;1:r1=0; 1:r3=0; 3:r1=0; 3:r3=1;
73052 :&gt;1:r1=1; 1:r3=0; 3:r1=0; 3:r3=1;
1     :&gt;1:r1=0; 1:r3=1; 3:r1=0; 3:r3=1;
42482 :&gt;1:r1=1; 1:r3=1; 3:r1=0; 3:r3=1;
90470 :&gt;1:r1=0; 1:r3=0; 3:r1=1; 3:r3=1;
30306 :&gt;1:r1=1; 1:r3=0; 3:r1=1; 3:r3=1;
43239 :&gt;1:r1=0; 1:r3=1; 3:r1=1; 3:r3=1;
205837:&gt;1:r1=1; 1:r3=1; 3:r1=1; 3:r3=1;
No

Witnesses
Positive: 0, Negative: 1000000
Condition exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0) is NOT validated
Hash=4fbfaafa51f6784d699e9bdaf5ba047d
Observation ppc-iriw-lwsync Never 0 1000000
Time ppc-iriw-lwsync 1.32
</pre><p>The non-SC outcome does not show up.</p><p>Altering parameters may yield this outcome.
In particular, we may try using all the available logical processors
with option <span class="c004">-a 64</span>.
Affinity control offers an alternative, which is enabled at compilation time
with <span class="c007">litmus7</span> option <span class="c004">-affinity</span>:
</p><pre class="verbatim">$ litmus7 ... -affinity incr1 ppc-iriw-lwsync.litmus -o ppc.tar
$ scp ppc.tar vargas:/var/tmp
</pre><p>Option <span class="c004">-affinity</span> takes one argument (<span class="c004">incr1</span> above)
that specifies the increment used while allocating
logical processors to test threads.
Here, the (POSIX) threads created by the test
(named <span class="c010">T</span><sub>0</sub>, <span class="c010">T</span><sub>1</sub>, <span class="c010">T</span><sub>2</sub> and <span class="c010">T</span><sub>3</sub> in Sec.&#XA0;<a href="#sec%3Aarch">2.1</a>)
will get bound to logical processors
0, 1, 2, and&#XA0;3, respectively.</p><p><a id="defi"></a>Namely, by default, the logical processors are
ordered as the sequence 0, 1, &#X2026;, <span class="c010">A</span>&#X2212;1 &#X2014;
where <span class="c010">A</span> is the number of available logical processors, which is
inferred by the test executable<sup><a id="text2" href="#note2">2</a></sup>.
Furthermore, logical processors are allocated to threads by
applying the affinity increment while scanning the logical processor sequence.
Observe that since the launch mode is changing (the default) threads
<span class="c010">T</span><sub><span class="c010">k</span></sub> correspond to different test threads&#XA0;<span class="c010">P</span><sub><span class="c010">i</span></sub> at each run.
The unpack compile and run sequence on <span class="c004">vargas</span> now yields
the non-SC outcome, better outcome variety and a lower running time:
</p><pre class="verbatim">vargas% tar xf /var/tmp/ppc.tar &amp;&amp; make
vargas% ./ppc-iriw-lwsync.exe
Test ppc-iriw-lwsync Allowed
Histogram (16 states)
180600:&gt;1:r1=0; 1:r3=0; 3:r1=0; 3:r3=0;
3656  :&gt;1:r1=1; 1:r3=0; 3:r1=0; 3:r3=0;
18812 :&gt;1:r1=0; 1:r3=1; 3:r1=0; 3:r3=0;
77692 :&gt;1:r1=1; 1:r3=1; 3:r1=0; 3:r3=0;
2973  :&gt;1:r1=0; 1:r3=0; 3:r1=1; 3:r3=0;
9     *&gt;1:r1=1; 1:r3=0; 3:r1=1; 3:r3=0;
28881 :&gt;1:r1=0; 1:r3=1; 3:r1=1; 3:r3=0;
75126 :&gt;1:r1=1; 1:r3=1; 3:r1=1; 3:r3=0;
20939 :&gt;1:r1=0; 1:r3=0; 3:r1=0; 3:r3=1;
30498 :&gt;1:r1=1; 1:r3=0; 3:r1=0; 3:r3=1;
1234  :&gt;1:r1=0; 1:r3=1; 3:r1=0; 3:r3=1;
89993 :&gt;1:r1=1; 1:r3=1; 3:r1=0; 3:r3=1;
75769 :&gt;1:r1=0; 1:r3=0; 3:r1=1; 3:r3=1;
76361 :&gt;1:r1=1; 1:r3=0; 3:r1=1; 3:r3=1;
87864 :&gt;1:r1=0; 1:r3=1; 3:r1=1; 3:r3=1;
229593:&gt;1:r1=1; 1:r3=1; 3:r1=1; 3:r3=1;
Ok

Witnesses
Positive: 9, Negative: 999991
Condition exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0) is validated
Hash=4fbfaafa51f6784d699e9bdaf5ba047d
Observation ppc-iriw-lwsync Sometimes 9 999991
Time ppc-iriw-lwsync 0.68
</pre><p>One may change the affinity increment with the command line option
<span class="c004">-i</span> of executable files. For instance, one binds the test threads
to logical processors 0, 2, 4 and&#XA0;6 as follows:
</p><pre class="verbatim">vargas% ./ppc-iriw-lwsync.exe -i 2 
Test ppc-iriw-lwsync Allowed
Histogram (15 states)
160629:&gt;1:r1=0; 1:r3=0; 3:r1=0; 3:r3=0;
33389 :&gt;1:r1=1; 1:r3=0; 3:r1=0; 3:r3=0;
43725 :&gt;1:r1=0; 1:r3=1; 3:r1=0; 3:r3=0;
93114 :&gt;1:r1=1; 1:r3=1; 3:r1=0; 3:r3=0;
33556 :&gt;1:r1=0; 1:r3=0; 3:r1=1; 3:r3=0;
64875 :&gt;1:r1=0; 1:r3=1; 3:r1=1; 3:r3=0;
34908 :&gt;1:r1=1; 1:r3=1; 3:r1=1; 3:r3=0;
43770 :&gt;1:r1=0; 1:r3=0; 3:r1=0; 3:r3=1;
64544 :&gt;1:r1=1; 1:r3=0; 3:r1=0; 3:r3=1;
4     :&gt;1:r1=0; 1:r3=1; 3:r1=0; 3:r3=1;
54633 :&gt;1:r1=1; 1:r3=1; 3:r1=0; 3:r3=1;
92617 :&gt;1:r1=0; 1:r3=0; 3:r1=1; 3:r3=1;
34754 :&gt;1:r1=1; 1:r3=0; 3:r1=1; 3:r3=1;
54027 :&gt;1:r1=0; 1:r3=1; 3:r1=1; 3:r3=1;
191455:&gt;1:r1=1; 1:r3=1; 3:r1=1; 3:r3=1;
No

Witnesses
Positive: 0, Negative: 1000000
Condition exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0) is NOT validated
Hash=4fbfaafa51f6784d699e9bdaf5ba047d
Observation ppc-iriw-lwsync Never 0 1000000
Time ppc-iriw-lwsync 0.92
</pre><p>One observes that the non-SC outcome does not show up
with the new affinity setting.</p><p>One may also bind test thread to logical processors randomly with
executable option <span class="c004">+ra</span>.
</p><pre class="verbatim">vargas% ./ppc-iriw-lwsync.exe +ra
Test ppc-iriw-lwsync Allowed
Histogram (15 states)
...
No

Witnesses
Positive: 0, Negative: 1000000
Condition exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0) is NOT validated
Hash=4fbfaafa51f6784d699e9bdaf5ba047d
Observation ppc-iriw-lwsync Never 0 1000000
Time ppc-iriw-lwsync 1.85
</pre><p>As we see, the condition does not validate either with random affinity.
As a matter of fact, logical processors are taken at random in the
sequence 0, 1, &#X2026;, 63;
while the successful run with <span class="c004">-i 1</span> took
them in the sequence 0, 1, 2, 3.
One can limit the sequence of logical processor with option <span class="c004">-p</span>,
which takes a sequence of logical processors numbers as argument:
</p><pre class="verbatim">vargas% ./ppc-iriw-lwsync.exe +ra -p 0,1,2,3
Test ppc-iriw-lwsync Allowed
Histogram (16 states)
...
8     *&gt;1:r1=1; 1:r3=0; 3:r1=1; 3:r3=0;
...
Ok

Witnesses
Positive: 8, Negative: 999992
Condition exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0) is validated
Hash=4fbfaafa51f6784d699e9bdaf5ba047d
Observation ppc-iriw-lwsync Sometimes 8 999992
Time ppc-iriw-lwsync 0.70
</pre><p>The condition now validates.</p>
<h4 class="subsubsection" id="sec11">2.2.2&#XA0;&#XA0;Study of affinity</h4>
<p>
As illustrated by the previous example, both the running time and the outcomes
of a test are sensitive to affinity settings.
We measured running time for increasing values of the affinity increment
from 0 (which disables affinity control)
to&#XA0;20, producing the following figure:
</p><div class="center"><img src="m1l-time.png"></div><p>
As regards outcome variety,
we get all of the 16 possible
outcomes only for an affinity increment of&#XA0;1.</p><p>The differences in running times can be explained by reference to the mapping
of logical processors to hardware.
The machine&#XA0;<span class="c004">vargas</span> consists in four MCM&#X2019;s (Multi-Chip-Module), each MCM
consists in four &#X201C;chips&#X201D;, each chip consists in two cores, and
each core may support two logical processors.
As far as we know, by querying <span class="c004">vargas</span>
with the AIX commands
<span class="c004">lsattr</span>, <span class="c004">bindprocessor</span>
and <span class="c004">llstat</span>,
the MCM&#X2019;s hold the logical processors
0&#X2013;15, 16&#X2013;31, 32&#X2013;47 and&#XA0;48&#X2013;63,
each chip holds the logical processors 4<span class="c010">k</span>, 4<span class="c010">k</span>+1, 4<span class="c010">k</span>+2, 4<span class="c010">k</span>+3
and each core holds the logical processors 2<span class="c010">k</span>, 2<span class="c010">k</span>+1.</p><p>The measure of running times for varying increments
reveals two noticeable slowdowns:
from an increment of&#XA0;1 to an increment of&#XA0;2 and from 5 to&#XA0;6.
The gap between&#XA0;1 and&#XA0;2 reveals the benefits of
SMT for our testing application.
An increment of&#XA0;1 yields both the greatest outcome
variety and the minimal running time.
The other gap may perhaps be explained by reference to MCM&#X2019;s:
for a value of&#XA0;5 the tests runs on the logical processors
0, 5, 10, 15, all belonging to the same
MCM; while the next affinity increment of&#XA0;6 results in
running the test on two different MCM (0, 6, 12 on the one hand
and 18 on the other).</p><p>As a conclusion, affinity control provides users with a certain level
of control over thread placement, which is likely to yield faster tests when
threads are constrained to run on logical processors that are &#X201C;close&#X201D; one
to another.
The best results are obtained when SMT is effectively enforced.
However, affinity control is no panacea, and the memory system may
be stressed by other means, such as, for instance, allocating important
chunks of memory (option&#XA0;<span class="c004">-s</span>).</p>
<h4 class="subsubsection" id="affinity:advanced">2.2.3&#XA0;&#XA0;Advanced control</h4>
<p>
For specific experiments, the technique of
allocating logical processors sequentially by following a fixed increment
may be two rigid. <span class="c007">litmus7</span> offers a finer control on affinity by allowing
users to supply the logical processors sequence.
Notice that most users will probably not need this advanced feature.</p><p>Anyhow, so as to confirm that testing <a href="ppc-iriw-lwsync"><span class="c004">ppc-iriw-lwsync</span></a>
benefits from not crossing chip boundaries, one may wish to confine
its four threads to logical processors 16 to&#XA0;19,
that is to the first chip of the second MCM.
This can be done by overriding the default logical processors sequence
by an user supplied one given as an argument to command-line
option&#XA0;<span class="c004">-p</span>:
</p><pre class="verbatim">vargas% ./ppc-iriw-lwsync.exe -p 16,17,18,19 -i 1
Test ppc-iriw-lwsync Allowed
Histogram (16 states)
169420:&gt;1:r1=0; 1:r3=0; 3:r1=0; 3:r3=0;
1287  :&gt;1:r1=1; 1:r3=0; 3:r1=0; 3:r3=0;
17344 :&gt;1:r1=0; 1:r3=1; 3:r1=0; 3:r3=0;
85329 :&gt;1:r1=1; 1:r3=1; 3:r1=0; 3:r3=0;
1548  :&gt;1:r1=0; 1:r3=0; 3:r1=1; 3:r3=0;
3     *&gt;1:r1=1; 1:r3=0; 3:r1=1; 3:r3=0;
27014 :&gt;1:r1=0; 1:r3=1; 3:r1=1; 3:r3=0;
75160 :&gt;1:r1=1; 1:r3=1; 3:r1=1; 3:r3=0;
19828 :&gt;1:r1=0; 1:r3=0; 3:r1=0; 3:r3=1;
29521 :&gt;1:r1=1; 1:r3=0; 3:r1=0; 3:r3=1;
441   :&gt;1:r1=0; 1:r3=1; 3:r1=0; 3:r3=1;
93878 :&gt;1:r1=1; 1:r3=1; 3:r1=0; 3:r3=1;
81081 :&gt;1:r1=0; 1:r3=0; 3:r1=1; 3:r3=1;
76701 :&gt;1:r1=1; 1:r3=0; 3:r1=1; 3:r3=1;
93623 :&gt;1:r1=0; 1:r3=1; 3:r1=1; 3:r3=1;
227822:&gt;1:r1=1; 1:r3=1; 3:r1=1; 3:r3=1;
Ok

Witnesses
Positive: 3, Negative: 999997
Condition exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0) is validated
Hash=4fbfaafa51f6784d699e9bdaf5ba047d
Observation ppc-iriw-lwsync Sometimes 3 999997
Time ppc-iriw-lwsync 0.63
</pre><p>Thus we get results similar to the previous experiment on logical processors
0 to&#XA0;3 (option <span class="c004">-i 1</span> alone).</p><p>We may also run four simultaneous instances (<span class="c004">-n 4</span>, parameter&#XA0;<span class="c010">n</span> of
section&#XA0;<a href="#sec%3Aarch">2.1</a>) of the test on
the four available MCM&#X2019;s:
</p><pre class="verbatim">vargas% ./ppc-iriw-lwsync.exe -p 0,1,2,3,16,17,18,19,32,33,34,35,48,49,50,51 -n 4 -i 1
Test ppc-iriw-lwsync Allowed
Histogram (16 states)
...
57    *&gt;1:r1=1; 1:r3=0; 3:r1=1; 3:r3=0;
...
Ok

Witnesses
Positive: 57, Negative: 3999943
Condition exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0) is validated
Hash=4fbfaafa51f6784d699e9bdaf5ba047d
Observation ppc-iriw-lwsync Sometimes 57 3999943
Time ppc-iriw-lwsync 0.75
</pre><p>Observe that, for a negligible penalty in running time, the number
of non-SC outcomes increases significantly.</p><p>By contrast, binding threads of a given instance of the test
to different MCM&#X2019;s results in poor running time and no non-SC outcome.
</p><pre class="verbatim">vargas% ./ppc-iriw-lwsync.exe -p 0,1,2,3,16,17,18,19,32,33,34,35,48,49,50,51 -n 4 -i 4
Test ppc-iriw-lwsync Allowed
Histogram (15 states)
...
Witnesses
Positive: 0, Negative: 4000000
Condition exists (0:r1=1 /\ 0:r3=0 /\ 2:r1=1 /\ 2:r3=0) is NOT validated
Time ppc-iriw-lwsync 1.48
</pre><p>In the experiment above, the increment is&#XA0;4, hence the logical processors
allocated to the first
instance of the test are 0, 16, 32, 48,
of which indices in the logical processors sequence are 0, 4, 8, 12,
respectively.
The next allocated index in the sequence is&#XA0;12+4 = 16.
However, the sequence has 16 items.
Wrapping around yields index&#XA0;0 which happens to be
the same as the starting index.
Then, so as to allocate fresh processors, the starting index is incremented
by one, resulting in allocating processors 1, 17, 33, 49
(indices 1, 5, 9, 13) to the second instance &#X2014; see section&#XA0;<a href="#incr%3Afull">2.3</a>
for the full story.
Similarly, the third and fourth instances will get processors
2, 18, 34, 50 and 3, 19, 35, 51, respectively.
Attentive readers may have noticed that the same experiment can
be performed with option <span class="c004">-i 16</span> and no <span class="c004">-p</span> option.</p><p>Finally, users should probably be aware that at least some versions of Linux
for x86 feature a less obvious mapping of logical processors to hardware.
On a bi-processor, dual-core, 2-ways hyper-threading, Linux, AMD64 machine,
we have checked that logical processors residing on the same core
are <span class="c010">k</span> and <span class="c010">k</span>+4, where <span class="c010">k</span> is an arbitrary core number ranging from
0 to&#XA0;3.
As a result, a proper choice for favouring effective hyper-threading
on such a machine is <span class="c004">-i 4</span> (or <span class="c004">-p 0,4,1,5,2,6,3,7 -i 1</span>).
More worthwhile noticing, perhaps,
the straightforward choice <span class="c004">-i 1</span> disfavours effective hyper-threading&#X2026;</p>
<h4 class="subsubsection" id="affinity:custom">2.2.4&#XA0;&#XA0;Custom control</h4>
<p>
Most tests run by <span class="c007">litmus7</span> are produced by the litmus test generators
described in Part&#XA0;<a href="gen.html#part%3Adiy">II</a>.
Those tests include meta-information that may direct affinity control.
For instance we generate one test with the
<span class="c007">diyone7</span> tool, see Sec.&#XA0;<a href="gen.html#diyone%3Aintro">5.2</a>.
More specifically we generate
<a href="IRIW+lwsyncs.litmus"><span class="c008">IRIW+lwsyncs</span></a> for Power
(<a href="ppc-iriw-lwsync.litmus"><span class="c008">ppc-iriw-lwsync</span></a> in the previous
section) as follows:
</p><pre class="verbatim">% diyone7 -arch PPC -name IRIW+lwsyncs Rfe LwSyncdRR Fre Rfe LwSyncdRR Fre
</pre><p>We get the new source file&#XA0;<a href="IRIW+lwsyncs.litmus"><span class="c004">IRIW+lwsyncs.litmus</span></a>:
</p><pre class="verbatim">PPC IRIW+lwsyncs
"Rfe LwSyncdRR Fre Rfe LwSyncdRR Fre"
Prefetch=0:x=T,1:x=F,1:y=T,2:y=T,3:y=F,3:x=T
Com=Rf Fr Rf Fr
Orig=Rfe LwSyncdRR Fre Rfe LwSyncdRR Fre
{
0:r2=x;
1:r2=x; 1:r4=y;
2:r2=y;
3:r2=y; 3:r4=x;
}
 P0           | P1           | P2           | P3           ;
 li r1,1      | lwz r1,0(r2) | li r1,1      | lwz r1,0(r2) ;
 stw r1,0(r2) | lwsync       | stw r1,0(r2) | lwsync       ;
              | lwz r3,0(r4) |              | lwz r3,0(r4) ;
exists
(1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0)
</pre><p>
The relevant meta-information is the &#X201C;<span class="c004">Com</span>&#X201D; line that describes
how test threads are related &#X2014; for instance, thread&#XA0;0 stores a value
to memory that is read by thread&#XA0;1, written &#X201C;<span class="c004">Rf</span>&#X201D; (see Part&#XA0;<a href="gen.html#part%3Adiy">II</a> for more details).
Custom affinity control will tend to run threads related by &#X201C;<span class="c004">Rf</span>&#X201D;
on &#X201C;close&#X201D; logical processors, where we can for instance consider
that close logical processors belong to the same physical core (SMT for Power).
This minimal logical processor topology is described by two <span class="c007">litmus7</span>
command-line option: <span class="c004">-smt &lt;n&gt;</span> that specifies <span class="c010">n</span>-way SMT;
and <span class="c004">-smt_mode (seq|end)</span> that specifies how logical processors
from the same core are numbered.
For a 8-cores 4-ways SMT power7 machine we invoke <span class="c007">litmus7</span> as follows:
</p><pre class="verbatim">% litmus7 -mem direct -smt 4 -smt_mode seq -affinity custom -o a.tar IRIW+lwsyncs.litmus
</pre><p>Notice that memory mode is direct and that the number of available
logical processors is unspecified, resulting in running one instance of
the test. More importantly, notice that affinity control is enabled
<span class="c004">-affinity custom</span>, additionally specifying custom affinity mode.</p><p>We then upload the archive <span class="c004">a.tar</span> to our Power7 machine,
unpack, compile and run the test:
</p><pre class="verbatim">power7% tar xmf a.tar
power7% make
...
power7% ./IRIW+lwsyncs.exe -v
./IRIW+lwsyncs.exe  -v
IRIW+lwsyncs: n=1, r=1000, s=1000, +rm, +ca, p='0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31'
thread allocation: 
[23,22,3,2] {5,5,0,0}
</pre><p>Option <span class="c004">-v</span> instructs the executable to show settings of the test harness: we see that one instance of the test is run (<span class="c004">n=1</span>),
size parameters are reminded (<span class="c004">r=1000, s=1000</span>) and
shuffling of indirect memory mode is performed (<span class="c004">+rm</span>).
Affinity settings are also given: mode is custom (<span class="c004">+ca</span>) and
the logical processor sequence inferred is given (<span class="c004">-p 0,1,&#X2026;,31</span>).
Additionally, the allocation of test threads to logical processors is
given, as <span class="c004">[&#X2026;]</span>, as well as the allocation of test threads
to physical cores, as <span class="c004">{&#X2026;}</span>.</p><p>Here is the run output proper:
</p><pre class="verbatim">Test IRIW+lwsyncs Allowed
Histogram (15 states)
2700  :&gt;1:r1=0; 1:r3=0; 3:r1=0; 3:r3=0;
142   :&gt;1:r1=1; 1:r3=0; 3:r1=0; 3:r3=0;
37110 :&gt;1:r1=0; 1:r3=1; 3:r1=0; 3:r3=0;
181257:&gt;1:r1=1; 1:r3=1; 3:r1=0; 3:r3=0;
78    :&gt;1:r1=0; 1:r3=0; 3:r1=1; 3:r3=0;
15    *&gt;1:r1=1; 1:r3=0; 3:r1=1; 3:r3=0;
103459:&gt;1:r1=0; 1:r3=1; 3:r1=1; 3:r3=0;
149486:&gt;1:r1=1; 1:r3=1; 3:r1=1; 3:r3=0;
30820 :&gt;1:r1=0; 1:r3=0; 3:r1=0; 3:r3=1;
9837  :&gt;1:r1=1; 1:r3=0; 3:r1=0; 3:r3=1;
2399  :&gt;1:r1=1; 1:r3=1; 3:r1=0; 3:r3=1;
204629:&gt;1:r1=0; 1:r3=0; 3:r1=1; 3:r3=1;
214700:&gt;1:r1=1; 1:r3=0; 3:r1=1; 3:r3=1;
5186  :&gt;1:r1=0; 1:r3=1; 3:r1=1; 3:r3=1;
58182 :&gt;1:r1=1; 1:r3=1; 3:r1=1; 3:r3=1;
Ok

Witnesses
Positive: 15, Negative: 999985
Condition exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0) is validated
Hash=836eb3085132d3cb06973469a08098df
Com=Rf Fr Rf Fr
Orig=Rfe LwSyncdRR Fre Rfe LwSyncdRR Fre
Affinity=[2, 3] [0, 1] ; (1,2) (3,0)
Observation IRIW+lwsyncs Sometimes 15 999985
Time IRIW+lwsyncs 0.70
</pre><p>As we see, the test validates. Namely we observe the non-SC behaviour of
<span class="c008">IRIW</span> in spite of the presence of two <span class="c004">lwsync</span> barriers.
We may also notice, in the executable output some meta-information related
to affinity: it reads that threads 2 and&#XA0;3 on the one hand and threads 0
and&#XA0;1 on the other are considered &#X201C;close&#X201D; (<em>i.e.</em> will run on the
same physical core); while threads 1 and&#XA0;2 on the one hand and threads 3 and&#XA0;0
on the other are considered &#X201C;far&#X201D; (<em>i.e.</em> will run on different cores).</p><p>Custom affinity can be disabled by enabling another affinity mode.
For instance with <span class="c004">-i 0</span> we specify an affinity increment of zero.
That is, affinity control is disabled altogether:
</p><pre class="verbatim">power7% ./IRIW+lwsyncs.exe -i 0 -v
./IRIW+lwsyncs.exe -i 0  -v
IRIW+lwsyncs: n=1, r=1000, s=1000, +rm, i=0, p='0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31'
Test IRIW+lwsyncs Allowed
Histogram (15 states)
...
No

Witnesses
Positive: 0, Negative: 1000000
Condition exists (1:r1=1 /\ 1:r3=0 /\ 3:r1=1 /\ 3:r3=0) is NOT validated
Hash=836eb3085132d3cb06973469a08098df
Com=Rf Fr Rf Fr
Orig=Rfe LwSyncdRR Fre Rfe LwSyncdRR Fre
Observation IRIW+lwsyncs Never 0 1000000
Time IRIW+lwsyncs 0.90
</pre><p>As we see, the test does not validate under those conditions.</p><p>Notice that section&#XA0;<a href="examples.html#affinity%3Aexperiment">17</a> describes a complete experiment
on affinity control.</p>
<h3 class="subsection" id="exec:control">2.3&#XA0;&#XA0;Controlling executable files</h3>
<h4 class="paragraph" id="sec15">Test conditions</h4>
<p>
Any executable file produced by <span class="c007">litmus7</span> accepts the following
command line options.
</p><dl class="description"><dt class="dt-description">
<span class="c006">-v</span></dt><dd class="dd-description"> Be verbose, can be repeated to increase verbosity.
Specifying <span class="c004">-v</span> is a convenient way to look at the default of options.
</dd><dt class="dt-description"><span class="c006">-q</span></dt><dd class="dd-description"> Be quiet.
</dd><dt class="dt-description"><span class="c006">-a &lt;n&gt;</span></dt><dd class="dd-description"> Run maximal number of tests concurrently for <span class="c010">n</span>&#XA0;available
logical processors &#X2014; parameter&#XA0;<span class="c010">a</span> in Sec.&#XA0;<a href="#defa">2.1</a>.
Notice that if affinity control is enabled (see below), <span class="c004">-a 0</span> will
set parameter&#XA0;<span class="c010">a</span> to the number of logical processors effectively available. 
</dd><dt class="dt-description"><span class="c006">-n &lt;n&gt;</span></dt><dd class="dd-description"> Run <span class="c010">n</span>&#XA0;tests concurrently &#X2014; parameter&#XA0;<span class="c010">n</span>
in Sec.&#XA0;<a href="#defn">2.1</a>. 
</dd><dt class="dt-description"><span class="c006">-r &lt;n&gt;</span></dt><dd class="dd-description"> Perform <span class="c010">n</span> runs &#X2014; parameter&#XA0;<span class="c010">r</span> in Sec.&#XA0;<a href="#defr">2.1</a>.
</dd><dt class="dt-description"><span class="c006">-fr &lt;f&gt;</span></dt><dd class="dd-description"> Multiply&#XA0;<span class="c010">r</span> by&#XA0;<span class="c010">f</span> (<span class="c010">f</span>&#XA0;is a floating point number).
</dd><dt class="dt-description"><span class="c006">-s &lt;n&gt;</span></dt><dd class="dd-description"> Size of a run &#X2014; parameter&#XA0;<span class="c010">s</span> in Sec.&#XA0;<a href="#defs">2.1</a>.
</dd><dt class="dt-description"><span class="c006">-fs &lt;f&gt;</span></dt><dd class="dd-description"> Multiply&#XA0;<span class="c010">s</span> by&#XA0;<span class="c010">f</span>.
</dd><dt class="dt-description"><span class="c006">-f &lt;f&gt;</span></dt><dd class="dd-description"> Multiply&#XA0;<span class="c010">s</span> by&#XA0;<span class="c010">f</span> and divide <span class="c010">r</span> by&#XA0;<span class="c010">f</span>.
</dd></dl><p>
<a id="generalized:integer"></a>Notice that options <span class="c004">-s</span> and&#XA0;<span class="c004">-r</span>
accept a generalised
syntax for their integer argument: when suffixed by&#XA0;<span class="c004">k</span> (resp.&#XA0;<span class="c004">M</span>)
the integer gets multiplied by&#XA0;10<sup>3</sup> (resp.&#XA0;10<sup>6</sup>).</p><p><a id="rm"></a>The following options are accepted only for tests compiled
in indirect memory mode (see Sec.&#XA0;<a href="#defmemorymode">2.1</a>):
</p><dl class="description"><dt class="dt-description">
<span class="c006">-rm</span></dt><dd class="dd-description"> Do not shuffle pointer arrays, resulting a behaviour
similar do direct mode, without recompilation.
</dd><dt class="dt-description"><span class="c006">+rm</span></dt><dd class="dd-description"> Shuffle pointer arrays, provided for regularity.
</dd></dl><p><a id="st"></a>The following option is accepted only for tests compiled
with a specified stride value (see Sec.&#XA0;<a href="#defstride">2.1</a>).
</p><dl class="description"><dt class="dt-description">
<span class="c006">-st &lt;n&gt;</span></dt><dd class="dd-description"> Change stride to&#XA0;<span class="c004">&lt;n&gt;</span>.
The default stride is specified at compile time by <span class="c007">litmus7</span>
option <span class="c004">-stride</span>.
</dd></dl><p>The following option is accepted when enabled at compile time:
</p><dl class="description"><dt class="dt-description">
<span class="c006">-l &lt;n&gt;</span></dt><dd class="dd-description">
Insert the assembly code of each thread in a loop of size <span class="c004">&lt;n&gt;</span>.
</dd></dl>
<h4 class="paragraph" id="sec16"><a id="affinity:runopt">Affinity</a></h4>
<p>
If affinity control has been enabled at compilation time
(for instance, by supplying option <span class="c004">-affinity incr1</span>
to <span class="c007">litmus7</span>),
the executable file produced by <span class="c007">litmus7</span> accepts the following
command line options.
</p><dl class="description"><dt class="dt-description">
<span class="c006">-p &lt;ns&gt;</span></dt><dd class="dd-description"> Logical processors sequence.
The sequence <span class="c004">&lt;ns&gt;</span> is a comma separated list of integers,
The default sequence is inferred by the executable as 0,1,&#X2026;,<span class="c010">A</span>&#X2212;1,
where <span class="c010">A</span> is the number of logical processors featured by the tested machine;
or is a sequence specified at compile time
with <span class="c007">litmus7</span> option&#XA0;<span class="c004">-p</span>.
</dd><dt class="dt-description"><span class="c006">-i &lt;n&gt;</span></dt><dd class="dd-description"> Increment for allocating logical processors to threads.
Default is specified at compile time by <span class="c007">litmus7</span> option&#XA0;<span class="c004">-affinity
incr&lt;<span class="c010">n</span>&gt;</span>.
Notice that <span class="c004">-i 0</span> disable affinity
and that <span class="c004">.exe</span> files reject the <span class="c004">-i</span> option when affinity
control has not been enabled at compile&#XA0;time.
</dd><dt class="dt-description"><span class="c006">+ra</span></dt><dd class="dd-description"> Perform random allocation of affinity at each test round.
</dd><dt class="dt-description"><span class="c006">+ca</span></dt><dd class="dd-description"> Perform custom affinity.
</dd></dl><p>
Notice that when custom affinity is not available, would it be that
the test source lacked meta-information or that logical processor
topology was not specified at compile-time, then <span class="c004">+ca</span>
behaves as <span class="c004">+ra</span>.</p><p><a id="incr:full"></a>Logical processors are allocated
test instance by test instance
(parameter&#XA0;<span class="c010">n</span> of Sec.&#XA0;<a href="#sec%3Aarch">2.1</a>) and then
thread by thread, scanning the logical processor sequence
left-to-right by steps of the given increment.
More precisely, assume a logical processor sequence
<span class="c010">P</span> = <span class="c010">p</span><sub>0</sub>, <span class="c010">p</span><sub>1</sub>, &#X2026;, <span class="c010">p</span><sub><span class="c010">A</span>&#X2212;1</sub> and an increment&#XA0;<span class="c010">i</span>.
The first processor allocated is <span class="c010">p</span><sub>0</sub>, then <span class="c010">p</span><sub><span class="c010">i</span></sub>, then <span class="c010">p</span><sub>2<span class="c010">i</span></sub> etc,
Indices in the sequence&#XA0;<span class="c010">P</span> are reduced modulo <span class="c010">A</span> so as to wrap around.
The starting index of the allocation sequence (initially 0) is recorded,
and coincidence with the index of the next processor to be 
allocated is checked.
When coincidence occurs, a new index is computed, as the previous
starting index plus one, which also becomes the new starting index.
Allocation then proceeds from this new starting index.
That way, all the processors in the sequence
will get allocated to different threads naturally, provided of
course that less than <span class="c010">A</span>&#XA0;threads are scheduled to run.
See section&#XA0;<a href="#affinity%3Aadvanced">2.2.3</a> for an example with <span class="c010">A</span>=16 and&#XA0;<span class="c010">i</span>=4.</p>
<h2 class="section" id="advanced:control">3&#XA0;&#XA0;Advanced control of test parameters</h2>
<h3 class="subsection" id="timebase">3.1&#XA0;&#XA0;Timebase synchronisation mode</h3>
<p>
Timebase synchronisation of the testing loop iterations
(see Sec.&#XA0;<a href="#timebase%3Aintro">2.1</a>) is selected by <span class="c007">litmus7</span> command line option
<span class="c004">-barrier timebase</span>.
In that mode,
test threads will first synchronise using polling synchronisation
barrier code, agree on a target timebase value and then loop
reading the timebase until it exceeds the target value.
Some tests demonstrate that timebase synchronisation
is more precise than user synchronisation (<span class="c004">-barrier user</span> and default).</p><p>For instance, consider the x86 test <a href="6.SB.litmus"><span class="c008">6.SB</span></a>,
a 6-thread analog of the <a href="#SB"><span class="c008">SB</span></a>&#XA0;test:
</p><pre class="verbatim">X86 6.SB
"Fre PodWR Fre PodWR Fre PodWR Fre PodWR Fre PodWR Fre PodWR"
{
}
 P0          | P1          | P2          | P3          | P4          | P5          ;
 MOV [x],$1  | MOV [y],$1  | MOV [z],$1  | MOV [a],$1  | MOV [b],$1  | MOV [c],$1  ;
 MOV EAX,[y] | MOV EAX,[z] | MOV EAX,[a] | MOV EAX,[b] | MOV EAX,[c] | MOV EAX,[x] ;
exists
(0:EAX=0 /\ 1:EAX=0 /\ 2:EAX=0 /\ 3:EAX=0 /\ 4:EAX=0 /\ 5:EAX=0)
</pre><p>
As for <a href="SB.litmus"><span class="c008">SB</span></a>, the final condition of
<a href="6.SB.litmus"><span class="c008">6.SB</span></a> identifies executions where each thread loads the initial
value&#XA0;0 of a location that is writtent into by another thread.
</p><div class="center"><img src="6.SB.png"></div><p>We first compile the test in user synchronisation mode, saving
<span class="c007">litmus7</span> output files into the directory&#XA0;<span class="c004">R</span>:
</p><pre class="verbatim">% mkdir -p R
% litmus7 -barrier user -vb true -o R 6.SB.litmus
% cd R
% make
</pre><p>The additional command line option <span class="c004">-vb true</span> activates the printing
of some timing information on synchronisations.</p><p>We then directly run the test executable <span class="c004">6.SB.exe</span>:
</p><pre class="verbatim">% ./6.SB.exe
Test 6.SB Allowed
Histogram (62 states)
7569  :&gt;0:EAX=1; 1:EAX=0; 2:EAX=0; 3:EAX=0; 4:EAX=0; 5:EAX=0;
8672  :&gt;0:EAX=0; 1:EAX=1; 2:EAX=0; 3:EAX=0; 4:EAX=0; 5:EAX=0;
...
326   :&gt;0:EAX=1; 1:EAX=0; 2:EAX=1; 3:EAX=1; 4:EAX=1; 5:EAX=1;
907   :&gt;0:EAX=0; 1:EAX=1; 2:EAX=1; 3:EAX=1; 4:EAX=1; 5:EAX=1;
No

Witnesses
Positive: 0, Negative: 1000000
Condition exists (0:EAX=0 /\ 1:EAX=0 /\ 2:EAX=0 /\ 3:EAX=0 /\ 4:EAX=0 /\ 5:EAX=0) is NOT validated
Hash=107f1303932972b3abace3ee4027408e
Observation 6.SB Never 0 1000000
Time 6.SB 0.85
</pre><p>The targeted outcome &#X2014; reading zero in the <code>EAX</code> registers
of the 6 threads &#X2014; is not observed.
We can observe synchronisation times for all tests runs
with the executable command line option <span class="c004">+vb</span>:
</p><pre class="verbatim">% ./6.SB.exe  +vb
99999: 162768 420978 564546   -894 669468
99998:    -93      3     81   -174   -651
99997:   -975    -30    -33     93   -192
99996:    990   1098    852   1176    774
...
</pre><p>We see five columns of numbers that list, for each test run,
the starting delays of <span class="c010">P</span><sub>1</sub>, <span class="c010">P</span><sub>2</sub> etc. with respect to <span class="c010">P</span><sub>0</sub>, expressed
in timebase ticks. Obviously, synchronisation is rather loose,
there are always two threads whose starting delays differ of
about 1000&#XA0;ticks.</p><p>We now compile the same test in timebase synchronisation mode,
saving <span class="c007">litmus7</span> output files into the pre-existing directory&#XA0;<span class="c004">RT</span>:
</p><pre class="verbatim">% mkdir -p RT
% litmus7 -barrier timebase -vb true -o RT 6.SB.litmus
% cd RT
% make
</pre><p>And we run the test directly
(option <span class="c004">-vb</span> disable the printing of any
synchronisation timing information):
</p><pre class="verbatim">% ./6.SB.exe -vb
Test 6.SB Allowed
Histogram (64 states)
60922 *&gt;0:EAX=0; 1:EAX=0; 2:EAX=0; 3:EAX=0; 4:EAX=0; 5:EAX=0;
38299 :&gt;0:EAX=1; 1:EAX=0; 2:EAX=0; 3:EAX=0; 4:EAX=0; 5:EAX=0;
...
598   :&gt;0:EAX=0; 1:EAX=1; 2:EAX=1; 3:EAX=1; 4:EAX=1; 5:EAX=1;
142   :&gt;0:EAX=1; 1:EAX=1; 2:EAX=1; 3:EAX=1; 4:EAX=1; 5:EAX=1;
Ok

Witnesses
Positive: 60922, Negative: 939078
Condition exists (0:EAX=0 /\ 1:EAX=0 /\ 2:EAX=0 /\ 3:EAX=0 /\ 4:EAX=0 /\ 5:EAX=0) is validated
Hash=107f1303932972b3abace3ee4027408e
Observation 6.SB Sometimes 60922 939078
Time 6.SB 1.62
</pre><p>We now see that the test validates. Moreover
all of the 64&#XA0;possible outcomes are observed.</p><p>Timebase synchronisation works as follows: at every iteration,
</p><ol class="enumerate" type=1><li class="li-enumerate">
one of the threads reads timebase&#XA0;<span class="c010">T</span>;
</li><li class="li-enumerate">all threads synchronise by the means of a polling synchronisation
barrier;
</li><li class="li-enumerate">each thread computes <span class="c010">T</span><sub><span class="c010">i</span></sub> = <span class="c010">T</span> + &#X3B4;<sub><span class="c010">i</span></sub>, where &#X3B4;<sub><span class="c010">i</span></sub> is
<em>the timebase delay</em>, a thread
specific constant;
</li><li class="li-enumerate">each thread loops, reading the timebase until the read value exceeds
<span class="c010">T</span><sub><span class="c010">i</span></sub>.
</li></ol><p>
By default the timebase delay &#X3B4;<sub><span class="c010">i</span></sub> is 2<sup>11</sup> = 2048 for all threads.</p><p>The precision of timebase synchronisation can be illustrated
by enabling the printing of all synchronisation timings:
</p><pre class="verbatim">% ./6.SB.exe +vb
99999: 672294[1] 671973[1] 672375[1] 672144[1] 672303[1] 672222[1]
99998:  4524[1]  4332[1]  4446[1]  2052[65]  2064[73]  4095[1]
...
99983:  4314[1]  3036[1]  3141[1]  2769[1]  4551[1]  3243[1]
99982:* 2061[36]  2064[33]  2067[11]  2079[12]  2064[14]  2064[24]
99981:  2121[1]  2382[1]  2586[1]  2643[1]  2502[1]  2592[1]
...
</pre><p>For each test iteration and each thread, two numbers are shown (1) the last
timebase value read by and&#XA0;(2) (in brackets <code>[</code>&#X2026;<code>]</code>) how many iterations of loop&#XA0;4. were performed.
Additionally a star &#X201C;<code>*</code>&#X201D; indicates the occurrence
of the targeted outcome.
Here, we see that a nearly perfect synchronisation can be achieved
(cf. line <code>99982:</code> above).</p><p>Once timebase synchronisation have been selected
(<span class="c007">litmus7</span> option <span class="c004">-barrier timebase</span>),
test executable behaviour can be altered by the following
two command line options:
</p><dl class="description"><dt class="dt-description">
<span class="c006">-ta &lt;n&gt;</span></dt><dd class="dd-description">
Change the timebase delay&#XA0;&#X3B4;<sub><span class="c010">i</span></sub> of all threads.
</dd><dt class="dt-description"><span class="c006">-tb &lt;0:n</span><sub>0</sub><span class="c006">;1:</span><span class="c010">n</span><sub>1</sub><span class="c006">;</span>&#X22EF;<span class="c006">&gt;</span></dt><dd class="dd-description">
Change the timebase delay&#XA0;&#X3B4;<sub><span class="c010">i</span></sub> of individual threads.
</dd></dl><p>The <span class="c007">litmus7</span> command line option&#XA0;<span class="c004">-vb true</span>
(verbose barrier)
governs the printing of synchronisation timings. It comes handy when
choosing values for the <span class="c004">-ta</span> and <span class="c004">-tb</span> options.
When set, the executable show synchronisation timings
for outcomes that validate the test final condition.
This default behaviour can be altered with
the following two command line options:
</p><dl class="description"><dt class="dt-description">
<span class="c006">-vb</span></dt><dd class="dd-description"> Do not show synchronisation timings.
</dd><dt class="dt-description"><span class="c006">+vb</span></dt><dd class="dd-description"> Show synchronisation timings for all outcomes.
</dd></dl><p>
Synchronisation timings are expressed in timebase ticks.
The format depends on the synchronisation mode
(<span class="c007">litmus7</span> option <span class="c004">-barrier</span>).
This section just gave two examples for user mode
(timings are show as differences from thread&#XA0;<span class="c010">P</span><sub>0</sub>); and for
timebase mode (timings are shown as differences
from a commonly agreed by all thread timebase value).
Notice that, when affinity control is enabled,
the running logical processors of threads are also shown.</p>
<h3 class="subsection" id="preload:custom">3.2&#XA0;&#XA0;Advanced prefetch control</h3>
<p>
Supplying the tags
<span class="c004">custom</span>, <span class="c004">static</span>, <span class="c004">static1</span> or&#XA0;<span class="c004">static2</span> to
<span class="c007">litmus7</span> command line option <span class="c004">-preload</span>
commands the insertion of cache prefetch or flush
instructions before every test instance.</p><p>In custom mode the execution of such cache management instruction
is under total user control, the other, &#X201C;static&#X201D;, modes offer
less control to the user, for the sake of not altering test code proper.</p>
<h4 class="subsubsection" id="sec20">3.2.1&#XA0;&#XA0;Custom prefetch</h4>
<p>
Custom prefetch mode offers complete control over
cache management instructions.
Users enable this mode by supplying the command line option
<span class="c004">-preload custom</span> to <span class="c007">litmus7</span>. For instance one may compile
the x86 test <a href="6.SB.litmus"><span class="c004">6.SB.litmus</span></a> as follows:
</p><pre class="verbatim">% mkdir -p R
% litmus7 -mem indirect -preload custom -o R 6.SB.litmus
% cd R
% make
</pre><p>Notice the test is compiled in <a href="#defmemorymode">indirect memory mode</a>,
in order to reduce false sharing effects.</p><p>The executable <span class="c004">6.SB.exe</span> accepts two new command line options:
<span class="c004">-prf</span> and&#XA0;<span class="c004">-pra</span>. Those options takes arguments that describe
cache management instructions.
The option <span class="c004">-pra</span> takes one letter that stands for a cache
management instruction as we here describe:
</p><div class="center">
<span class="c004">I</span>: do nothing,
<span class="c004">F</span>: cache flush,
<span class="c004">T</span>: cache touch,
<span class="c004">W</span>: cache touch for a write.
</div><p>
All those cache management instructions are not provided by all architectures,
in case some instruction is missing, the letters behave as follows:
</p><div class="center">
<span class="c004">F</span>: do nothing,
<span class="c004">T</span>: do nothing,
<span class="c004">W</span>: behave as <span class="c004">T</span>.
</div><p>With <span class="c004">-pra </span><span class="c010">X</span> the commanded action applies to all
threads and all variables, for instance:
</p><pre class="verbatim">% ./6.SB.exe -pra T
</pre><p>will perform a run where every test thread touches the test locations
that it refers to
(<em>i.e.</em> <span class="c004">x</span> and&#XA0;<span class="c004">y</span> for Thread&#XA0;0, <span class="c004">y</span>
and <span class="c004">z</span> for Thread&#XA0;1, etc.)
before executing test code proper.
Although one may achieve interesting results by using
this <span class="c004">-pra</span> option, the more selective
<span class="c004">-prf</span> option should prove more useful.
The <span class="c004">-prf</span> option takes a comma separated list of cache managment
directives.
A cache management directive is <span class="c010">n</span>:<span class="c010">loc</span>=<span class="c010">X</span>,
where <span class="c010">n</span> is a thread number, <span class="c010">loc</span> is a program variable,
and <span class="c010">X</span> is a cache management controle letter.
For instance, <span class="c004">-prf 0:y=T</span> instructs
thread&#XA0;0 to touch location&#XA0;<span class="c004">y</span>.
More generally, having each thread of the test
<a href="6.SB.litmus"><span class="c008">6.SB</span></a> to touch the memory location
it reads with its second instruction would favor reading the initial value
of these locations,
and thus validating
the final condition of the test 
&#X201C;<code>(0:EAX=0 /\ 1:EAX=0 /\ 2:EAX=0 /\ 3:EAX=0 /\ 4:EAX=0 /\ 5:EAX=0)</code>&#X201D;.</p><p>Notice that those locations can be found by looking
at the <a href="6.SB.litmus">test code</a>
or at the <a href="6.SB.png">diagram</a> of the target execution.
Let us have a try:
</p><pre class="verbatim">./6.SB.exe -prf 0:y=T,1:z=T,2:a=T,3:b=T,4:c=T,5:x=T               
Test 6.SB Allowed
Histogram (63 states)
10    *&gt;0:EAX=0; 1:EAX=0; 2:EAX=0; 3:EAX=0; 4:EAX=0; 5:EAX=0;
...
Witnesses
Positive: 10, Negative: 999990
...
Prefetch=0:y=T,1:z=T,2:a=T,3:b=T,4:c=T,5:x=T
...
</pre><p>As can be seen, the final condition is validated. Also notice
that the prefetch directives used during the run are reminded.
If given several times, <span class="c004">-prf</span> options cumulate,
the rightmost directives taking precedence in case of ambiguity.
As a consequence, one may achieve the same prefetching
effect as above with:
</p><pre class="verbatim">% ./6.SB.exe -prf 0:y=T -prf 1:z=T -prf 2:a=T -prf 3:b=T -prf 4:c=T -prf 5:x=T
</pre>
<h4 class="subsubsection" id="sec21">3.2.2&#XA0;&#XA0;Prefetch metadata</h4>
<p>
The source code of tests may include prefetch directives as metadata
prefixed with &#X201C;<code>Prefetch=</code>&#X201D;.
In particular, the generators of the <span class="c007">diy7</span>
suite (see Part&#XA0;<a href="gen.html#part%3Adiy">II</a>) produce such metadata.
For instance in the case of the
<span class="c008">6.SB</span> test (generated source <a href="6.SB+Prefetch.litmus"><span class="c004">6.SB+Prefetch.litmus</span></a>),
this metadata reads:
</p><pre class="verbatim">Prefetch=0:x=F,0:y=T,1:y=F,1:z=T,2:z=F,2:a=T,3:a=F,3:b=T,4:b=F,4:c=T,5:c=F,5:x=T
</pre><p>That is, each thread flushes the location it stores to and touches
each location it reads from.
Notice that each thread starts with a memory location access
(here a store) and ends with another (here a load).
The idea simply is to accelerate the exit access (with a cache touch)
while delaying the entry access (with a cache flush).</p><p>When prefetch metadata is available, it acts as the default of
prefetch directives:
</p><pre class="verbatim">% litmus7 -mem indirect -preload custom -o R 6.SB+Prefetch.litmus
% cd R
% make
</pre><p>Then we run the test by:
</p><pre class="verbatim">% ./6.SB+Prefetch.exe
Test 6.SB Allowed
Histogram (63 states)
674   *&gt;0:EAX=0; 1:EAX=0; 2:EAX=0; 3:EAX=0; 4:EAX=0; 5:EAX=0;
...
Witnesses
Positive: 674, Negative: 999326
...
Prefetch=0:x=F,0:y=T,1:y=F,1:z=T,2:a=T,2:z=F,3:a=F,3:b=T,4:b=F,4:c=T,5:c=F,5:x=T
...
</pre><p>One may notice that the prefetch directives from the source file
medata found its way to the test executable.</p><p>As with any kind of metadata,
one can change the prefetch metadata by editing the litmus source file,
or better by using the <span class="c004">-hints</span> command line option.
The <span class="c004">-hints</span> command line option takes a filename as argument.
This file is a <a href="#defmapping">mapping</a> that associates
new metadata to test names.
As an example, we reverse <span class="c007">diy7</span> scheme for cache management directives:
accelerating entry accesses and delaying exit accesses:
</p><pre class="verbatim">% cat map.txt
6.SB Prefetch=0:x=W,0:y=F,1:y=W,1:z=F,2:a=F,2:z=W,3:a=W,3:b=F,4:b=W,4:c=F,5:c=W,5:x=F
% litmus7 -mem indirect -preload custom -hints map.txt -o R 6.SB.litmus
% cd R
% make 
...
% ./6.SB.exe 
Test 6.SB Allowed
Histogram (63 states)
24    *&gt;0:EAX=0; 1:EAX=0; 2:EAX=0; 3:EAX=0; 4:EAX=0; 5:EAX=0;
...
Prefetch=0:x=W,0:y=F,1:y=W,1:z=F,2:a=F,2:z=W,3:a=W,3:b=F,4:b=W,4:c=F,5:c=W,5:x=F
...
</pre><p>As we see above, the final condition validates.
It does so in spite of
the apparently unfavourable cache management directives.</p><p>We can experiment further without recompilation, by
using the <span class="c004">-pra</span> and&#XA0;<span class="c004">-prf</span> command line options of
the test executable. Those are parsed left-to-right, so that we can
(1) cancel any default cache management directive with
<span class="c004">-pra I</span> and&#XA0;(2) enable cache touch for the stores:
</p><pre class="verbatim">% ./6.SB.exe -pra I -prf 0:x=W -prf 1:y=W -prf 2:z=W -prf 3:a=W -prf 4:b=W -prf 5:c=W
Test 6.SB Allowed
...
Witnesses
Positive: 0, Negative: 1000000
...
Prefetch=0:x=W,1:y=W,2:z=W,3:a=W,4:b=W,5:c=W
</pre><p>As we see, the final condition does not validate.</p><p>By contrast, flushing or touching the locations that the threads load permit to
repetitively achieve validation:
</p><pre class="verbatim">chi% ./6.SB.exe -pra I -prf 0:y=F -prf 1:z=F -prf 2:a=F -prf 3:b=F -prf 4:c=F -prf 5:x=F
Test 6.SB Allowed
Histogram (63 states)
211   *&gt;0:EAX=0; 1:EAX=0; 2:EAX=0; 3:EAX=0; 4:EAX=0; 5:EAX=0;
...
% ./6.SB.exe -pra I -prf 0:y=T -prf 1:z=T -prf 2:a=T -prf 3:b=T -prf 4:c=T -prf 5:x=T
Test 6.SB Allowed
Histogram (63 states)
10    *&gt;0:EAX=0; 1:EAX=0; 2:EAX=0; 3:EAX=0; 4:EAX=0; 5:EAX=0;
...
</pre><p>As a conclusion, interpreting the impact of cache management
directives is not easy. However, custom preload mode
(litmus command line option <span class="c004">-preload custom</span>) and test executable
options <span class="c004">-pra</span> and&#XA0;<span class="c004">-prf</span> allow experimentation on specific tests.</p>
<h4 class="subsubsection" id="sec22">3.2.3&#XA0;&#XA0;&#X201C;Static&#X201D; prefetch control</h4>
<p>
Custom prefetch mode comes handy when one wants to tailor cache management
directives for a particular test.
In practice, we run batches of tests using source metadata for
prefetch directives.
In such a setting, the code that interprets the
prefetch directives is useless,
as we do not use the <span class="c004">-prf</span> option of the test executables.
As this code get executed before each test thread code, it may impact test
results.
It is desirable to supress this code from test executables, still
performing cache management instructions.
To that aim, <span class="c007">litmus7</span> provides some &#X201C;static&#X201D; preload modes, enabled
with command line options <span class="c004">-preload static</span>,
<span class="c004">-preload static1</span> and&#XA0;<span class="c004">-preload static2</span>.</p><p>In the former mode <span class="c004">-preload static</span> and without any further user
intervention, each test thread executes the cache management
instructions commanded by the <code>Prefetch</code> metadata:
</p><pre class="verbatim">% mkdir -p S
% litmus7 -mem indirect -preload static -o R 6.SB+Prefetch.litmus
% make -C S
% S/6.SB+Prefetch.exe
Test 6.SB Allowed
Histogram (63 states)
804   *&gt;0:EAX=0; 1:EAX=0; 2:EAX=0; 3:EAX=0; 4:EAX=0; 5:EAX=0;
...
Observation 804 999196
...
</pre><p>As we can see above, the effect of the cache management
instructions looks more favorable than in custom preload mode.</p><p>Users still have a limited control on the
execution of cache management instructions: produced executable accept
a new <span class="c004">-prs &lt;n&gt;</span> option, which take a positive or null integer as argument.
Then, each test thread executes the cache
management instructions commanded by source metadata with probability&#XA0;1/<span class="c010">n</span>,
the special value <span class="c010">n</span>=0 disabling prefetch altogether.
The default for the <span class="c004">-prs</span> options is &#X201C;<span class="c004">1</span>&#X201D; (always execute
the cache management instructions).
Let us try:
</p><pre class="verbatim">% S/6.SB+Prefetch.exe -prs 0 | grep Observation
Observation 6.SB Never 0 1000000
% S/6.SB+Prefetch.exe -prs 1 | grep Observation
Observation 6.SB Sometimes 901 999099
% S/6.SB+Prefetch.exe -prs 2 | grep Observation
Observation 6.SB Sometimes 29 999971
% S/6.SB+Prefetch.exe -prs 3 | grep Observation
Observation 6.SB Sometimes 16 999984
</pre><p>In those experiments we show the &#X201C;<code>Observation</code>&#X201D; field of <span class="c007">litmus7</span>
output: this field gives the count of outcomes that validate the final
condition, followed by the count of outcomes that do not validate
the final condition. The above counts confirm that
cache management instructions favor validation.</p><p>The remaining preload modes <span class="c004">static1</span> and&#XA0;<span class="c004">static2</span> are similar,
except that they produce executable files that do not accept
the <span class="c004">-prs</span> option. Furthermore, in the former
mode <span class="c004">-preload static1</span> cache management instructions are always executed,
while in the latter mode <span class="c004">-preload static2</span>
cache management instructions are executed with probability&#XA0;1/2.
Those modes thus act as pure static mode
(<span class="c007">litmus7</span> option <span class="c004">-preload static</span>),
with runtime options <span class="c004">-prs 1</span> and&#XA0;<span class="c004">-prs 2</span> respectively.
Moreover,
as the test scaffold includes no code to interpret the <span class="c004">-prs &lt;n&gt;</span>
switch, the test code is less perturbed.
In practice and for the <span class="c008">6.SB</span> example, there is little difference:
</p><pre class="verbatim">% mkdir -p S1 S2
% litmus7 -mem indirect -preload static1 -o S1 6.SB+Prefetch.litmus
% litmus7 -mem indirect -preload static2 -o S2 6.SB+Prefetch.litmus
% make -C S1 &amp;&amp; make -C S2
...
% S1/6.SB+Prefetch.exe | grep Observation
Observation 6.SB Sometimes 1119 998881
% S2/6.SB+Prefetch.exe | grep Observation
Observation 6.SB Sometimes 16 999984
</pre>
<h2 class="section" id="sec23">4&#XA0;&#XA0;Usage of <span class="c007">litmus7</span></h2>
<h3 class="subsection" id="sec24">Arguments</h3>
<p>
<span class="c007">litmus7</span> takes file names as command line arguments.
Those files are either a single litmus test,
when having extension <span class="c004">.litmus</span>,
or a list of file names, when prefixed by <span class="c004">@</span>.
Of course, the file names in <span class="c004">@</span>files can themselves be
<span class="c004">@</span>files.</p><h3 class="subsection" id="sec25">Options</h3>
<p>
There are many command line options.
We describe the more useful ones:</p><h4 class="paragraph" id="sec26">General behaviour</h4>
<dl class="description"><dt class="dt-description">
<span class="c006">-version</span></dt><dd class="dd-description"> Show version number and exit.
</dd><dt class="dt-description"><span class="c006">-libdir</span></dt><dd class="dd-description"> Show installation directory and exit.
</dd><dt class="dt-description"><span class="c006">-v</span></dt><dd class="dd-description"> Be verbose, can be repeated to increase verbosity.
</dd><dt class="dt-description"><span class="c006">-mach &lt;name&gt;</span></dt><dd class="dd-description"> Read configuration file&#XA0;<span class="c004">name.cfg</span>.
See the <a href="#config%3Asec">next section</a>
for the syntax of configuration files.
</dd><dt class="dt-description"><span class="c006">-o &lt;dest&gt;</span></dt><dd class="dd-description">
Save C-source of test files into <span class="c004">&lt;dest&gt;</span> instead of running them.
If argument <span class="c004">&lt;dest&gt;</span> is an archive (extension <span class="c004">.tar</span>)
or a compressed archive (extension <span class="c004">.tgz</span>),
<span class="c007">litmus7</span> builds an archive: this is the &#X201C;cross&#XA0;compilation feature&#X201D;
demonstrated in Sec.&#XA0;<a href="#cross">1.2</a>.
Otherwise, <span class="c004">&lt;dest&gt;</span> is interpreted as the name of an
existing directory and tests are saved in it.
</dd><dt class="dt-description"><span class="c006">-driver (shell|C|XCode)</span></dt><dd class="dd-description">
Choose the driver that will run the tests.
In the &#X201C;<span class="c004">shell</span>&#X201D; (and default) mode,
each test will be compiled into an executable. A dedicated shell script
<span class="c004">run.sh</span> will launch the test executables.
In the &#X201C;<span class="c004">C</span>&#X201D; mode, one executable <span class="c004">run.exe</span> is produced, which
will launch the tests.
Finally, the <span class="c004">XCode</span> mode is for inclusion of the tests into
a dedicated iOS App, which we do not distribute at the moment.
</dd><dt class="dt-description"><span class="c006">-crossrun &lt;(user@)?host(:port)?|adb|qemu&gt;</span></dt><dd class="dd-description">
When the shell driver is used (<span class="c004">-driver shell</span> above),
the first two possible arguments <a id="crossrun">instruct</a>
the <span class="c004">run.sh</span> script to run individual tests on a remote machine.
The remote host can be contacted by the means of <span class="c004">ssh</span>
or the Android Debug Bridge.
<dl class="description"><dt class="dt-description">
<span class="c006">ssh</span></dt><dd class="dd-description">
<span class="c004">user</span> is a login name on the the remote host,
<span class="c004">&lt;host&gt;</span> is the name of the remote host,
and <span class="c004">port</span> is a port-number which can be omitted when standard&#XA0;(22).
</dd><dt class="dt-description"><span class="c006">adb</span></dt><dd class="dd-description">
Tests will be run in the remote directory <span class="c004">/data/tmp</span>.
</dd></dl>
This option may be useful when the tested machine has little disk space or a
crippled installation.
Default is disabled &#X2014; <em>i.e.</em> run tests on
the machine where the <span class="c004">run.sh</span> script runs.<p>The argument <span class="c004">qemu</span> instruct the shell script to use the <span class="c004">qemu</span>
emulator. The <span class="c004">QEMU</span> environment variable must be defined to the
emulator path before running the <span class="c004">run.sh</span> script, as in (<span class="c004">sh</span> syntax): <code>QEMU=qemu sh run.sh</code>.</p></dd><dt class="dt-description"><span class="c006">-index &lt;@name&gt;</span></dt><dd class="dd-description"> Save the source names of compiled files in index
file&#XA0;<span class="c004">@name</span>.
</dd></dl><h4 class="paragraph" id="litmus:option:sec">Test conditions</h4>
<p>
The following options set the default values of
the options of the executable files produced:
</p><dl class="description"><dt class="dt-description">
<a id="avail"><span class="c006">-a &lt;n&gt;</span></a></dt><dd class="dd-description">Run maximal number of tests concurrently for <span class="c010">n</span>&#XA0;available
logical processors &#X2014;
set default value for <span class="c004">-a</span> of Sec.&#XA0;<a href="#exec%3Acontrol">2.3</a>.
Default is&#XA0;1 (run one test).
When affinity control is enabled, the value&#XA0;0 has the special
meaning of having executables to set the
number of available logical processors according to
how many are actually present.
</dd><dt class="dt-description"><span class="c006">-limit &lt;bool&gt;</span></dt><dd class="dd-description"> Do not process tests with more than&#XA0;<span class="c010">n</span>
threads, where <span class="c010">n</span> is the number of available cores defined above.
Default is <span class="c004">true</span>.
</dd><dt class="dt-description"><span class="c006">-r &lt;n&gt;</span></dt><dd class="dd-description"> Perform <span class="c010">n</span> runs &#X2014; set default value for
option&#XA0;<span class="c004">-r</span> of Sec.&#XA0;<a href="#exec%3Acontrol">2.3</a>.
The option accepts generalised syntax for integers and
default is&#XA0;10.
</dd><dt class="dt-description"><a id="sizeoftest"><span class="c006">-s &lt;n&gt;</span></a></dt><dd class="dd-description">Size of a run &#X2014; set default value for
option <span class="c004">-s</span> of Sec.&#XA0;<a href="#exec%3Acontrol">2.3</a>.
The option accepts generalised syntax for integers and
default is 100000 (or <span class="c004">100k</span>).
</dd></dl><p>The following additional options control the various modes described
in Sec.&#XA0;<a href="#sec%3Aarch">2.1</a>, and more.
Those cannot be changed without running <span class="c007">litmus7</span> again:
</p><dl class="description"><dt class="dt-description">
<span class="c006">-barrier (user|userfence|pthread|none|timebase)</span></dt><dd class="dd-description"> Set synchronisation mode, default <span class="c004">user</span>. Synchronisation modes are described
in Sec.&#XA0;<a href="#defsynchronisation">2.1</a>
</dd><dt class="dt-description"><span class="c006">-launch (changing|fixed)</span></dt><dd class="dd-description"> Set launch mode,
default <span class="c004">changing</span>.
</dd><dt class="dt-description"><span class="c006">-mem (indirect|direct)</span></dt><dd class="dd-description"> Set memory mode,
default <span class="c004">indirect</span>.
It is possible to instruct executables compiled in indirect mode
to behave almost as if compiled in direct mode, see Sec.&#XA0;<a href="#rm">2.3</a>.
</dd><dt class="dt-description"><span class="c006">-stride &lt;n&gt;</span></dt><dd class="dd-description">
Specify a stride value of <span class="c004">&lt;n&gt;</span> &#X2014; set default value for option <span class="c004">-st</span> of Sec.&#XA0;<a href="#st">2.3</a>. See Sec.&#XA0;<a href="#defstride">2.1</a> for details on the stride parameter.
If &lt;n&gt; is negative or zero, restore the default,
which is stride feature disabled.
</dd><dt class="dt-description"><span class="c006">-st &lt;n&gt;</span></dt><dd class="dd-description"> Alias for <span class="c004">-stride &lt;n&gt;</span>.
</dd><dt class="dt-description"><span class="c006">-para (self|shell)</span></dt><dd class="dd-description">
Perform several tests concurrently, either by forking POSIX
threads (as described in Sec.&#XA0;<a href="#sec%3Aarch">2.1</a>), or by forking
Unix processes. Only applies for cross compilation.
Default is <span class="c004">self</span>.
</dd><dt class="dt-description"><span class="c006">-alloc (dynamic|static|before)</span></dt><dd class="dd-description">
Set memory allocation mode. In &#X201C;dynamic&#X201D; and &#X201C;before&#X201D; modes, the memory
used by test threads is allocated with <span class="c004">malloc</span> &#X2014; in &#X201C;before&#X201D; mode,
memory is allocated before forking test instances.
In &#X201C;static&#X201D; mode, the memory is pre-allocated as static arrays.
In that latter case, the size of allocated arrays depend upon
compile time defined parameters: the number of available logical processors
(see option <a href="#avail"><span class="c004">-a &lt;n&gt;</span></a>)
and the size of a run (see option <a href="#sizeoftest"><span class="c004">-s &lt;n&gt;</span></a>).
It remains possible to change those those at execution time, provided
the resulting memory size does not exceed the compile time value.
Default is <span class="c004">dynamic</span>.</dd><dt class="dt-description"><span class="c006">-preload (no|random|custom|static|static1|static2)</span></dt><dd class="dd-description">
Specify preload mode (see Sec.&#XA0;<a href="#defpreload">2.1</a>), default is <span class="c004">random</span>.
Starting from version 5.0 we provide additional &#X201C;custom&#X201D; and &#X201C;static&#X201D;
modes for a finer control of prefetching and flushing of some memory locations
by some threads. See Sec&#XA0;<a href="#preload%3Acustom">3.2</a>.
</dd><dt class="dt-description"><span class="c006">-safer (no|all|write)</span></dt><dd class="dd-description"> Specify safer mode,
default is <span class="c004">write</span>.
When instructed to do so, executable files perform some consistency checks.
Those are intended both for debugging and for dynamically checking
some assumptions on POSIX threads that we rely upon.
More specifically the test harness checks for the stabilisation of
memory locations after a test round in the &#X201C;<span class="c004">all</span>&#X201D; and
&#X201C;<span class="c004">write</span>&#X201D; mode, while
the initial values of memory locations are checked in &#X201C;<span class="c004">all</span>&#X201D; mode.
</dd><dt class="dt-description"><span class="c006">-speedcheck (no|some|all)</span></dt><dd class="dd-description"> 
Quick condition check mode,
default is &#X201C;<span class="c004">no</span>&#X201D;.
In mode &#X201C;<span class="c004">some</span>&#X201D;, test executable will stop as soon as its
condition is settled.
In mode &#X201C;<span class="c004">all</span>&#X201D;, the <span class="c004">run.sh</span> script will additionally
not run the test if invoked once more later.
</dd></dl><p><a id="affinity:control">The</a> following optiondra commands affinity control:
</p><dl class="description"><dt class="dt-description">
<span class="c006">-affinity (none|incr&lt;n&gt;|random|custom)</span></dt><dd class="dd-description">
Enable (of disable with tag <span class="c004">none</span>) affinity control,
specifying default affinity mode of executables.
Default is <span class="c004">none</span>, <em>i.e.</em> executables do not
include affinity control code.
The various tags are interpreted as follows:
<ol class="enumerate" type=1><li class="li-enumerate">
<span class="c004">incr&lt;n&gt;</span>:
integer <span class="c004">&lt;n&gt;</span> is the increment for allocating logical
processors to threads &#X2014; see Sec.&#XA0;<a href="#sec%3Aaffinity">2.2</a>.
Notice that with <span class="c004">-affinity incr0</span>
the produced code features affinity control, which executable
files do not exercise by default.
</li><li class="li-enumerate"><span class="c004">random</span>: executables perform random allocation of
test threads to logical processors.
</li><li class="li-enumerate"><span class="c004">custom</span>: executables perform custom allocation of
test threads to logical processors.
</li></ol>
Notice that the default for executables can be overridden using
options <span class="c004">-i</span>,<span class="c004">+ra</span> and&#XA0;<span class="c004">+ca</span>
of Sec.&#XA0;<a href="#exec%3Acontrol">2.3</a>.</dd><dt class="dt-description"><span class="c006">-i &lt;n&gt;</span></dt><dd class="dd-description"> Alias for <span class="c004">-affinity incr&lt;n&gt;</span>.
</dd></dl><p>
Notice that affinity control is not implemented for MacOs.</p><p>The following options are significant when affinity control is enabled.
Otherwise they are silent no-ops.
</p><dl class="description"><dt class="dt-description">
<span class="c006">-p &lt;ns&gt;</span></dt><dd class="dd-description">
Specify the sequence of logical processors.
The notation <span class="c004">&lt;ns&gt;</span> stands for a comma separated list of integers.
Set default value for option <span class="c004">-p</span> of Sec.&#XA0;<a href="#exec%3Acontrol">2.3</a>.
Default for this <span class="c004">-p</span> option
will let executable files compute the logical processor sequence
themselves.
</dd><dt class="dt-description"><span class="c006">-force_afffinity &lt;bool&gt;</span></dt><dd class="dd-description">
Code that sets affinity will spin until all specified
cores (as given with option <span class="c004">-avail &lt;n&gt;</span>) processors
are up. This option is necessary on devices that let core sleep
when the computing load is low. Default is false.
</dd></dl><p>Custom affinity control (see Sec.&#XA0;<a href="#affinity%3Acustom">2.2.4</a>) is enabled,
first by enabling affinity control (<em>e.g.</em> with <span class="c004">-affinity &#X2026;</span>),
and then by specifying a logical processor topology with options <span class="c004">-smt</span>
and <span class="c004">-smt_mode</span>.
</p><dl class="description"><dt class="dt-description">
<span class="c006">-smt &lt;n&gt;</span></dt><dd class="dd-description"> Specify that logical processors are close by groups
of <span class="c010">n</span>, default is <span class="c004">1</span>.
</dd><dt class="dt-description"><span class="c006">-smt_mode (none|seq|end)</span></dt><dd class="dd-description"> Specify how &#X201C;close&#X201D; logical processors
are numbered, default is <span class="c004">none</span>.
In mode&#XA0;&#X201C;<span class="c004">end</span>&#X201D;, logical processors of the same core
are numbered as <span class="c010">c</span>, <span class="c010">c</span>+<span class="c010">A</span><sub><span class="c010">c</span></sub> etc. where <span class="c010">c</span> is a physical core number and
<span class="c010">A</span><sub><span class="c010">c</span></sub> is the number of physical cores available.
In mode&#XA0;&#X201C;<span class="c004">seq</span>&#X201D;, logical processors of the same core
are numbered in sequence.
</dd></dl><p>
Notice that custom affinity works only for those tests that include the proper
meta-information. Otherwise, custom affinity silently degrades
to random affinity.</p><p>Finally, a few miscellaneous options are documented:
</p><dl class="description"><dt class="dt-description">
<span class="c006">-l &lt;n&gt;</span></dt><dd class="dd-description">
Insert the assembly code of each thread in test in a loop of size <span class="c004">&lt;n&gt;</span>.
Accepts generalised integer syntax, disabled by default.
Sets default value for option <span class="c004">-l</span> of Sec.&#XA0;<a href="#exec%3Acontrol">2.3</a>.<p>This feature may prove useful for measuring running times that are not
too much perturbed by the test harness, in combination
with options&#XA0;<span class="c004">-s 1 -r 1</span>.
</p></dd><dt class="dt-description"><span class="c006">-vb &lt;bool&gt;</span></dt><dd class="dd-description">
Disable/enable the printing of synchronisation timings, default is <span class="c004">false</span>.<p>This feature may prove useful for analysing the synchronisation behaviour of
a specific test, see Sec.&#XA0;<a href="#timebase">3.1</a>.
</p></dd><dt class="dt-description"><span class="c006">-ccopts &lt;flags&gt;</span></dt><dd class="dd-description"> Set <span class="c007">gcc</span> compilation flags
(defaults: X86=<span class="c004">"-fomit-frame-pointer -O2"</span>, PPC/ARM=<span class="c004">"-O2"</span>).
</dd><dt class="dt-description"><a id="gcc"><span class="c006">-gcc &lt;name&gt;</span></a></dt><dd class="dd-description">
Change the name of C compiler, default <span class="c004">gcc</span>.
</dd><dt class="dt-description"><a id="linkopt"><span class="c006">-linkopt &lt;flags&gt;</span></a></dt><dd class="dd-description"> Set <span class="c007">gcc</span> linking flags.
(default: void).
</dd><dt class="dt-description"><a id="gas"><span class="c006">-gas &lt;bool&gt;</span></a></dt><dd class="dd-description">
Emit Gnu as extensions (default Linux/Mac=<span class="c004">true</span>, AIX=<span class="c004">false</span>)
</dd></dl><h4 class="paragraph" id="sec28">Target architecture description</h4>
<p>
Litmus compilation chain may slightly vary depending on the following
parameters:
</p><dl class="description"><dt class="dt-description">
<span class="c006">-os (linux|mac|aix)</span></dt><dd class="dd-description"> Set target operating system.
This parameter mostly impacts some of <span class="c007">gcc</span> options. Default <span class="c004">linux</span>.
</dd><dt class="dt-description"><span class="c006">-ws (w32|w64)</span></dt><dd class="dd-description"> Set word size.
This option first selects <span class="c007">gcc</span> 32 or&#XA0;64 bits mode,
by providing it with the appropriate option (<span class="c004">-m32</span>
or <span class="c004">-m64</span> on linux, <span class="c004">-maix32</span>
or <span class="c004">-maix64</span> on AIX).
It also slightly impacts code generation in the corner case
where memory locations hold other memory locations.
Default is a bit contrived: it acts as <span class="c004">w32</span> as regards code
generation, while it provides no 32/64 bits mode selection option
to&#XA0;<span class="c004">gcc</span>.
</dd></dl><h4 class="paragraph" id="change:input">Change input</h4>
<p>
Some items in
the source of tests can be changed at the very last moment.
<a id="defmapping">The</a>
new items are defined in mapping files whose names are arguments to
the appropriate command line options.
Mapping files simply are lists of pairs, with one line starting with a test
name, and the rest of line defining the changed item.
The changed item may also contains several lines: in that case it should be
included in double quotes&#XA0;&#X201C;<span class="c004">"</span>.&#X201D;.
</p><dl class="description"><dt class="dt-description">
<span class="c006">-names &lt;file&gt;</span></dt><dd class="dd-description"> Run <span class="c007">litmus7</span> only on tests whose names are
listed in <span class="c004">&lt;file&gt;</span>.
</dd><dt class="dt-description"><span class="c006">-rename &lt;file&gt;</span></dt><dd class="dd-description"> Change test names.
</dd><dt class="dt-description"><span class="c006">-kinds &lt;file&gt;</span></dt><dd class="dd-description"> Change test kinds.
This amonts to changing the quantifier of final conditions, with
kind <span class="c004">Allow</span> being <code>exists</code>,
kind <span class="c004">Forbid</span> being <code>~exists</code>
and kind <span class="c004">Require</span> being <code>forall</code>.
</dd><dt class="dt-description"><span class="c006">-conds &lt;file&gt;</span></dt><dd class="dd-description"> Change the final condition of tests.
</dd><dt class="dt-description"><span class="c006">-hints &lt;file&gt;</span></dt><dd class="dd-description"> Change meta-data, or hints.
Hints command avanced features such as custom affinity
(option <span class="c004">-affinity custom</span> and Sec.&#XA0;<a href="#affinity%3Acustom">2.2.4</a>)
and prefech control
(option <span class="c004">-preload custom</span> and Sec.&#XA0;<a href="#preload%3Acustom">3.2</a>).</dd></dl><p>
Observe that the rename mapping is applied first. As a result kind or condition
change must refer to new names. For instance, we can highlight
that a X86 machine is not sequentially consistent by first
renaming <a href="SB.litmus"><span class="c008">SB</span></a> into <span class="c008">SB+SC</span>, and then changing the
final condition.
The new condition expresses
that the first instruction (a store)
of one of the threads must come first:
</p><div class="center">
<table class="c001 cellpading0"><tr><td class="c015"><a href="rename.txt"><span class="c004">rename.txt</span></a></td><td class="c023">&nbsp;</td><td class="c015"><a href="cond.txt"><span class="c004">cond.txt</span></a></td></tr>
<tr><td class="hbar" colspan=3></td></tr>
<tr><td class="c023"><pre class="verbatim">SB SB+SC
</pre></td><td class="c023">&#XA0;&#XA0;&#XA0;&#XA0;&#XA0;&#XA0;</td><td class="c023"><pre class="verbatim">SB+SC "forall (0:EAX=1 \/ 1:EAX=1)"
</pre></td></tr>
</table>
</div><p>
Then, we run litmus:
</p><pre class="verbatim">% litmus7 -mach x86 -rename rename.txt -conds cond.txt SB.litmus
%%%%%%%%%%%%%%%%%%%%%%%%%
% Results for SB.litmus %
%%%%%%%%%%%%%%%%%%%%%%%%%
X86 SB+SC
"Fre PodWR Fre PodWR"

{x=0; y=0;}

 P0          | P1          ;
 MOV [x],$1  | MOV [y],$1  ;
 MOV EAX,[y] | MOV EAX,[x] ;

forall (0:EAX=1 \/ 1:EAX=1)
Generated assembler
#START _litmus_P1
        movl $1,(%r8,%rdx)
        movl (%rdx),%eax
#START _litmus_P0
        movl $1,(%rdx)
        movl (%r8,%rdx),%eax

Test SB+SC Required
Histogram (4 states)
39954 *&gt;0:EAX=0; 1:EAX=0;
3979407:&gt;0:EAX=1; 1:EAX=0;
3980444:&gt;0:EAX=0; 1:EAX=1;
195   :&gt;0:EAX=1; 1:EAX=1;
No

Witnesses
Positive: 7960046, Negative: 39954
Condition forall (0:EAX=1 \/ 1:EAX=1) is NOT validated
Hash=7dbd6b8e6dd4abc2ef3d48b0376fb2e3
Observation SB+SC Sometimes 7960046 39954
Time SB+SC 0.48
</pre><p>One sees that the test name and final condition have changed.</p><h4 class="paragraph" id="misc">Miscellaneous</h4>
<dl class="description"><dt class="dt-description">
<span class="c006">-sleep &lt;n&gt;</span></dt><dd class="dd-description"> Insert a delay of <span class="c010">n</span> seconds between each individual test run.
</dd><dt class="dt-description"><span class="c006">-exit &lt;bool&gt;</span></dt><dd class="dd-description"> Exit status of each individual test executable reflects the final condition success or failure, default <span class="c004">false</span> (test exit status always is success in absence of errors).
</dd></dl><h3 class="subsection" id="sec31"><a id="config:sec">Configuration</a> files</h3>
<p>
The syntax of configuration files is minimal:
lines &#X201C;<span class="c010">key</span> <span class="c004">=</span> <span class="c010">arg</span>&#X201D; are interpreted
as setting the value of parameter&#XA0;<span class="c010">key</span> to <span class="c010">arg</span>.
Each parameter has a corresponding option,
usually <span class="c004">-</span><span class="c010">key</span>, except for single-letter options:
</p><div class="center">

<table class="c001 cellpading0"><tr><td class="hbar" colspan=3></td></tr>
<tr><td class="hbar" colspan=3></td></tr>
<tr><td class="c015">&#XA0;&#XA0;<span class="c010">option</span>&#XA0;&#XA0;</td><td class="c015">&#XA0;&#XA0;<span class="c010">key</span>&#XA0;&#XA0;</td><td class="c015">&#XA0;&#XA0;<span class="c010">arg</span>&#XA0;&#XA0; </td></tr>
<tr><td class="hbar" colspan=3></td></tr>
<tr><td class="hbar" colspan=3></td></tr>
<tr><td class="c017"><span class="c004">-a</span></td><td class="c017">avail</td><td class="c017">integer </td></tr>
<tr><td class="c017"><span class="c004">-s</span></td><td class="c017">size_of_test</td><td class="c017">integer</td></tr>
<tr><td class="c017"><span class="c004">-r</span></td><td class="c017">number_of_run</td><td class="c017">integer</td></tr>
<tr><td class="c017"><span class="c004">-p</span></td><td class="c017">procs</td><td class="c017">list of integers</td></tr>
<tr><td class="c017"><span class="c004">-l</span></td><td class="c017">loop</td><td class="c017">integer</td></tr>
<tr><td class="hbar" colspan=3></td></tr>
<tr><td class="hbar" colspan=3></td></tr>
</table>
</div><p>
Notice that <span class="c007">litmus7</span> in fact accepts long versions of options
(<em>e.g.</em> <span class="c004">-avail</span> for&#XA0;<span class="c004">-a</span>).</p><p>As command line option are processed left-to-right,
settings from a configuration file (option <span class="c004">-mach</span>)
can be overridden by a later
command line option.
Some configuration files for the machines we have tested
are present in the distribution. As an example here is the configuration
file <span class="c004">hpcx.cfg</span>.
</p><pre class="verbatim">size_of_test = 2000
number_of_run = 20000
os = AIX
ws = W32
# A node has 16 cores X2 (SMT)
avail = 32
</pre><p>
Lines introduced by <code>#</code> are comments and are thus ignored.</p><p>Configuration files are searched first in the current directory;
then in any directory specified
by setting the shell environment variable <span class="c004">LITMUSDIR</span>;
and then in litmus installation directory, which is defined
while compiling&#XA0;<span class="c007">litmus7</span>.
</p>
<hr class="ffootnoterule"><dl class="thefootnotes"><dt class="dt-thefootnotes">
<a id="note1" href="#text1">1</a></dt><dd class="dd-thefootnotes"><div class="footnotetext">Power and x86-based systems
provide a user accessible timebase counter that should provide
consistent times to all cores and processors.</div></dd><dt class="dt-thefootnotes"><a id="note2" href="#text2">2</a></dt><dd class="dd-thefootnotes"><div class="footnotetext">Parameter <span class="c010">A</span> is not to be confused with&#XA0;<span class="c010">a</span> of section&#XA0;<a href="#sec%3Aarch">2.1</a>. The former serves to compute logical threads while the latter governs the number of tests that run simultaneously. However
parameters&#XA0;<span class="c010">a</span> will be set to&#XA0;<span class="c010">A</span> when affinity control is enabled and when
<span class="c010">a</span>&#XA0;value is&#XA0;0.</div></dd></dl>
<hr>
<a href="diy001.html"><img src="previous_motif.gif" alt="Previous"></a>
<a href="index.html"><img src="contents_motif.gif" alt="Up"></a>
<a href="gen.html"><img src="next_motif.gif" alt="Next"></a>
</body>
</html>
